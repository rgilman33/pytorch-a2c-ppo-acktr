{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from envs import make_env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=False\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=16\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=8e6\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.vis_interval=100\n",
    "        self.load_model=False\n",
    "        self.save_model=True\n",
    "        \n",
    "args = args()\n",
    "\n",
    "SAVE_PATH = \"saved_models/a2c_121717.pt\"\n",
    "LOAD_PATH = \"saved_models/a2c_121717.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 80, FPS 206, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.77068, value loss 0.03095, policy loss -0.20477\n",
      "Updates 100, num timesteps 8080, FPS 220, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.78782, value loss 0.08699, policy loss -0.06560\n",
      "Updates 200, num timesteps 16080, FPS 199, mean/median reward -14.2/-21.0, min/max reward -21.0/0.0, entropy 1.78581, value loss 0.03604, policy loss 0.06901\n",
      "Updates 300, num timesteps 24080, FPS 201, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78591, value loss 0.21487, policy loss -0.34689\n",
      "Updates 400, num timesteps 32080, FPS 201, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78901, value loss 0.13538, policy loss -0.20581\n",
      "Updates 500, num timesteps 40080, FPS 202, mean/median reward -20.2/-21.0, min/max reward -21.0/-19.0, entropy 1.78800, value loss 0.07956, policy loss -0.09811\n",
      "Updates 600, num timesteps 48080, FPS 201, mean/median reward -20.0/-20.0, min/max reward -21.0/-17.0, entropy 1.76683, value loss 0.03538, policy loss 0.04355\n",
      "Updates 700, num timesteps 56080, FPS 201, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78529, value loss 0.07816, policy loss -0.07393\n",
      "Updates 800, num timesteps 64080, FPS 201, mean/median reward -20.2/-20.0, min/max reward -21.0/-17.0, entropy 1.79102, value loss 0.09895, policy loss -0.10066\n",
      "Updates 900, num timesteps 72080, FPS 201, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.79096, value loss 0.00238, policy loss 0.07918\n",
      "Updates 1000, num timesteps 80080, FPS 201, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78903, value loss 0.00287, policy loss 0.08643\n",
      "Updates 1100, num timesteps 88080, FPS 201, mean/median reward -20.5/-21.0, min/max reward -21.0/-18.0, entropy 1.78184, value loss 0.04647, policy loss 0.11593\n",
      "Updates 1200, num timesteps 96080, FPS 201, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79087, value loss 0.03469, policy loss 0.02199\n",
      "Updates 1300, num timesteps 104080, FPS 201, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79103, value loss 0.13291, policy loss -0.18778\n",
      "Updates 1400, num timesteps 112080, FPS 201, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.79137, value loss 0.12033, policy loss -0.15885\n",
      "Updates 1500, num timesteps 120080, FPS 201, mean/median reward -19.6/-20.0, min/max reward -21.0/-17.0, entropy 1.78789, value loss 0.04501, policy loss 0.02668\n",
      "Updates 1600, num timesteps 128080, FPS 201, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78414, value loss 0.06442, policy loss 0.01352\n",
      "Updates 1700, num timesteps 136080, FPS 201, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78807, value loss 0.04288, policy loss 0.12950\n",
      "Updates 1800, num timesteps 144080, FPS 200, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78491, value loss 0.14524, policy loss -0.24957\n",
      "Updates 1900, num timesteps 152080, FPS 200, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79091, value loss 0.04834, policy loss -0.03168\n",
      "Updates 2000, num timesteps 160080, FPS 200, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78998, value loss 0.11174, policy loss 0.07961\n",
      "Updates 2100, num timesteps 168080, FPS 200, mean/median reward -20.5/-21.0, min/max reward -21.0/-18.0, entropy 1.78618, value loss 0.09077, policy loss 0.03973\n",
      "Updates 2200, num timesteps 176080, FPS 200, mean/median reward -19.9/-21.0, min/max reward -21.0/-15.0, entropy 1.75749, value loss 0.07342, policy loss 0.20762\n",
      "Updates 2300, num timesteps 184080, FPS 200, mean/median reward -19.9/-20.0, min/max reward -21.0/-15.0, entropy 1.78650, value loss 0.25968, policy loss 0.15770\n",
      "Updates 2400, num timesteps 192080, FPS 201, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78615, value loss 0.20299, policy loss 0.17750\n",
      "Updates 2500, num timesteps 200080, FPS 201, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79008, value loss 0.11980, policy loss -0.13217\n",
      "Updates 2600, num timesteps 208080, FPS 201, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78909, value loss 0.08699, policy loss -0.01459\n",
      "Updates 2700, num timesteps 216080, FPS 198, mean/median reward -19.9/-21.0, min/max reward -21.0/-18.0, entropy 1.77729, value loss 0.07806, policy loss 0.05210\n",
      "Updates 2800, num timesteps 224080, FPS 198, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79061, value loss 0.04766, policy loss -0.05856\n",
      "Updates 2900, num timesteps 232080, FPS 197, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78781, value loss 0.01359, policy loss 0.07034\n",
      "Updates 3000, num timesteps 240080, FPS 197, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79010, value loss 0.03320, policy loss -0.03646\n",
      "Updates 3100, num timesteps 248080, FPS 197, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78949, value loss 0.02291, policy loss 0.05130\n",
      "Updates 3200, num timesteps 256080, FPS 197, mean/median reward -20.6/-21.0, min/max reward -21.0/-18.0, entropy 1.78823, value loss 0.03026, policy loss 0.05888\n",
      "Updates 3300, num timesteps 264080, FPS 197, mean/median reward -20.1/-21.0, min/max reward -21.0/-18.0, entropy 1.78965, value loss 0.02539, policy loss -0.03500\n",
      "Updates 3400, num timesteps 272080, FPS 197, mean/median reward -20.0/-21.0, min/max reward -21.0/-18.0, entropy 1.78876, value loss 0.07637, policy loss -0.00213\n",
      "Updates 3500, num timesteps 280080, FPS 197, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79040, value loss 0.03346, policy loss 0.00675\n",
      "Updates 3600, num timesteps 288080, FPS 197, mean/median reward -20.3/-21.0, min/max reward -21.0/-17.0, entropy 1.78974, value loss 0.05244, policy loss -0.04447\n",
      "Updates 3700, num timesteps 296080, FPS 196, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.79050, value loss 0.02714, policy loss -0.05397\n",
      "Updates 3800, num timesteps 304080, FPS 196, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79102, value loss 0.00636, policy loss -0.02309\n",
      "Updates 3900, num timesteps 312080, FPS 196, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79093, value loss 0.05557, policy loss 0.18042\n",
      "Updates 4000, num timesteps 320080, FPS 196, mean/median reward -20.1/-21.0, min/max reward -21.0/-18.0, entropy 1.79146, value loss 0.01704, policy loss -0.05019\n",
      "Updates 4100, num timesteps 328080, FPS 196, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79059, value loss 0.03339, policy loss 0.09070\n",
      "Updates 4200, num timesteps 336080, FPS 196, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.79033, value loss 0.02073, policy loss 0.00761\n",
      "Updates 4300, num timesteps 344080, FPS 196, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79057, value loss 0.05275, policy loss 0.02054\n",
      "Updates 4400, num timesteps 352080, FPS 196, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.78769, value loss 0.04957, policy loss -0.04723\n",
      "Updates 4500, num timesteps 360080, FPS 196, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78999, value loss 0.02217, policy loss 0.02457\n",
      "Updates 4600, num timesteps 368080, FPS 196, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78696, value loss 0.03728, policy loss -0.00277\n",
      "Updates 4700, num timesteps 376080, FPS 195, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79149, value loss 0.01567, policy loss -0.00989\n",
      "Updates 4800, num timesteps 384080, FPS 195, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78999, value loss 0.01562, policy loss 0.05728\n",
      "Updates 4900, num timesteps 392080, FPS 195, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.79129, value loss 0.00949, policy loss 0.01357\n",
      "Updates 5000, num timesteps 400080, FPS 195, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79080, value loss 0.02798, policy loss 0.04093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 5100, num timesteps 408080, FPS 195, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79094, value loss 0.02377, policy loss 0.06227\n",
      "Updates 5200, num timesteps 416080, FPS 194, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.78993, value loss 0.01621, policy loss -0.00158\n",
      "Updates 5300, num timesteps 424080, FPS 193, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79101, value loss 0.01132, policy loss 0.03668\n",
      "Updates 5400, num timesteps 432080, FPS 193, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.79050, value loss 0.01713, policy loss -0.11173\n",
      "Updates 5500, num timesteps 440080, FPS 194, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79078, value loss 0.05533, policy loss 0.11098\n",
      "Updates 5600, num timesteps 448080, FPS 193, mean/median reward -20.6/-21.0, min/max reward -21.0/-20.0, entropy 1.78958, value loss 0.01724, policy loss -0.04666\n",
      "Updates 5700, num timesteps 456080, FPS 193, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.79080, value loss 0.01702, policy loss -0.07007\n",
      "Updates 5800, num timesteps 464080, FPS 193, mean/median reward -20.1/-21.0, min/max reward -21.0/-18.0, entropy 1.79012, value loss 0.01252, policy loss -0.02550\n",
      "Updates 5900, num timesteps 472080, FPS 193, mean/median reward -20.1/-21.0, min/max reward -21.0/-17.0, entropy 1.79106, value loss 0.01800, policy loss -0.01634\n",
      "Updates 6000, num timesteps 480080, FPS 193, mean/median reward -20.1/-20.0, min/max reward -21.0/-17.0, entropy 1.78625, value loss 0.01764, policy loss -0.02304\n",
      "Updates 6100, num timesteps 488080, FPS 193, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78952, value loss 0.01817, policy loss 0.03858\n",
      "Updates 6200, num timesteps 496080, FPS 193, mean/median reward -20.4/-20.0, min/max reward -21.0/-20.0, entropy 1.78929, value loss 0.02821, policy loss -0.07073\n",
      "Updates 6300, num timesteps 504080, FPS 193, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79008, value loss 0.00629, policy loss 0.02933\n",
      "Updates 6400, num timesteps 512080, FPS 193, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78983, value loss 0.02430, policy loss 0.04764\n",
      "Updates 6500, num timesteps 520080, FPS 192, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.79017, value loss 0.01861, policy loss 0.05634\n",
      "Updates 6600, num timesteps 528080, FPS 192, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78953, value loss 0.00789, policy loss -0.03745\n",
      "Updates 6700, num timesteps 536080, FPS 192, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.79045, value loss 0.01489, policy loss 0.01630\n",
      "Updates 6800, num timesteps 544080, FPS 192, mean/median reward -20.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78829, value loss 0.01811, policy loss -0.00767\n",
      "Updates 6900, num timesteps 552080, FPS 192, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.79050, value loss 0.05363, policy loss 0.16519\n",
      "Updates 7000, num timesteps 560080, FPS 192, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78930, value loss 0.00786, policy loss 0.06762\n",
      "Updates 7100, num timesteps 568080, FPS 192, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.79060, value loss 0.03892, policy loss -0.13693\n",
      "Updates 7200, num timesteps 576080, FPS 192, mean/median reward -20.1/-20.0, min/max reward -21.0/-17.0, entropy 1.79096, value loss 0.02230, policy loss 0.02120\n",
      "Updates 7300, num timesteps 584080, FPS 192, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78886, value loss 0.01189, policy loss -0.08057\n",
      "Updates 7400, num timesteps 592080, FPS 192, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.79060, value loss 0.01167, policy loss -0.08174\n",
      "Updates 7500, num timesteps 600080, FPS 192, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79033, value loss 0.03295, policy loss -0.01386\n",
      "Updates 7600, num timesteps 608080, FPS 192, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.79059, value loss 0.03419, policy loss 0.06735\n",
      "Updates 7700, num timesteps 616080, FPS 192, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.78773, value loss 0.02435, policy loss -0.14064\n",
      "Updates 7800, num timesteps 624080, FPS 191, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.78843, value loss 0.01191, policy loss -0.01339\n",
      "Updates 7900, num timesteps 632080, FPS 191, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.79119, value loss 0.07181, policy loss -0.17454\n",
      "Updates 8000, num timesteps 640080, FPS 191, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78992, value loss 0.02136, policy loss -0.04111\n",
      "Updates 8100, num timesteps 648080, FPS 191, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79069, value loss 0.02344, policy loss -0.08833\n",
      "Updates 8200, num timesteps 656080, FPS 191, mean/median reward -20.1/-21.0, min/max reward -21.0/-17.0, entropy 1.79099, value loss 0.01418, policy loss -0.02922\n",
      "Updates 8300, num timesteps 664080, FPS 191, mean/median reward -20.5/-21.0, min/max reward -21.0/-18.0, entropy 1.79045, value loss 0.01952, policy loss -0.07801\n",
      "Updates 8400, num timesteps 672080, FPS 191, mean/median reward -20.1/-21.0, min/max reward -21.0/-18.0, entropy 1.78738, value loss 0.02202, policy loss -0.05668\n",
      "Updates 8500, num timesteps 680080, FPS 191, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79116, value loss 0.02022, policy loss 0.03546\n",
      "Updates 8600, num timesteps 688080, FPS 191, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78907, value loss 0.02750, policy loss -0.00777\n",
      "Updates 8700, num timesteps 696080, FPS 191, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79070, value loss 0.10581, policy loss 0.08830\n",
      "Updates 8800, num timesteps 704080, FPS 191, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.79034, value loss 0.03531, policy loss 0.08059\n",
      "Updates 8900, num timesteps 712080, FPS 191, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78962, value loss 0.02092, policy loss -0.02056\n",
      "Updates 9000, num timesteps 720080, FPS 191, mean/median reward -19.5/-20.0, min/max reward -21.0/-17.0, entropy 1.79005, value loss 0.07387, policy loss -0.13981\n",
      "Updates 9100, num timesteps 728080, FPS 191, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79102, value loss 0.03629, policy loss -0.07303\n",
      "Updates 9200, num timesteps 736080, FPS 191, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78920, value loss 0.03781, policy loss 0.02846\n",
      "Updates 9300, num timesteps 744080, FPS 190, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78847, value loss 0.03889, policy loss 0.10348\n",
      "Updates 9400, num timesteps 752080, FPS 190, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78896, value loss 0.01135, policy loss 0.01317\n",
      "Updates 9500, num timesteps 760080, FPS 190, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.78794, value loss 0.03509, policy loss -0.04791\n",
      "Updates 9600, num timesteps 768080, FPS 190, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.79047, value loss 0.00391, policy loss -0.00939\n",
      "Updates 9700, num timesteps 776080, FPS 189, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78988, value loss 0.02173, policy loss -0.05374\n",
      "Updates 9800, num timesteps 784080, FPS 189, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78884, value loss 0.03006, policy loss -0.00510\n",
      "Updates 9900, num timesteps 792080, FPS 190, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78711, value loss 0.01652, policy loss 0.01980\n",
      "Updates 10000, num timesteps 800080, FPS 190, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78306, value loss 0.01383, policy loss -0.04846\n",
      "Updates 10100, num timesteps 808080, FPS 189, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78968, value loss 0.02224, policy loss -0.04027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 10200, num timesteps 816080, FPS 189, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78739, value loss 0.04248, policy loss 0.03589\n",
      "Updates 10300, num timesteps 824080, FPS 189, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.79005, value loss 0.03869, policy loss -0.10848\n",
      "Updates 10400, num timesteps 832080, FPS 189, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.78896, value loss 0.01236, policy loss -0.08232\n",
      "Updates 10500, num timesteps 840080, FPS 189, mean/median reward -20.1/-20.0, min/max reward -21.0/-17.0, entropy 1.78935, value loss 0.00907, policy loss 0.00505\n",
      "Updates 10600, num timesteps 848080, FPS 189, mean/median reward -20.1/-20.0, min/max reward -21.0/-16.0, entropy 1.79001, value loss 0.01721, policy loss -0.01120\n",
      "Updates 10700, num timesteps 856080, FPS 189, mean/median reward -20.0/-20.0, min/max reward -21.0/-16.0, entropy 1.78693, value loss 0.04935, policy loss 0.15373\n",
      "Updates 10800, num timesteps 864080, FPS 189, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.78638, value loss 0.01030, policy loss 0.04003\n",
      "Updates 10900, num timesteps 872080, FPS 189, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79008, value loss 0.01669, policy loss -0.06451\n",
      "Updates 11000, num timesteps 880080, FPS 189, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78874, value loss 0.01753, policy loss -0.10329\n",
      "Updates 11100, num timesteps 888080, FPS 189, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78734, value loss 0.01256, policy loss 0.01824\n",
      "Updates 11200, num timesteps 896080, FPS 189, mean/median reward -19.6/-20.0, min/max reward -21.0/-17.0, entropy 1.78970, value loss 0.02476, policy loss 0.09623\n",
      "Updates 11300, num timesteps 904080, FPS 189, mean/median reward -20.0/-20.0, min/max reward -21.0/-19.0, entropy 1.78471, value loss 0.00605, policy loss -0.00152\n",
      "Updates 11400, num timesteps 912080, FPS 189, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78368, value loss 0.03742, policy loss 0.01195\n",
      "Updates 11500, num timesteps 920080, FPS 188, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78760, value loss 0.01588, policy loss 0.04576\n",
      "Updates 11600, num timesteps 928080, FPS 188, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79045, value loss 0.04042, policy loss -0.19572\n",
      "Updates 11700, num timesteps 936080, FPS 188, mean/median reward -19.8/-20.0, min/max reward -21.0/-15.0, entropy 1.78715, value loss 0.03680, policy loss 0.03584\n",
      "Updates 11800, num timesteps 944080, FPS 188, mean/median reward -20.1/-21.0, min/max reward -21.0/-15.0, entropy 1.78604, value loss 0.01504, policy loss -0.02096\n",
      "Updates 11900, num timesteps 952080, FPS 188, mean/median reward -19.8/-20.0, min/max reward -21.0/-15.0, entropy 1.78726, value loss 0.01441, policy loss 0.00820\n",
      "Updates 12000, num timesteps 960080, FPS 188, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78758, value loss 0.01865, policy loss -0.01925\n",
      "Updates 12100, num timesteps 968080, FPS 188, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78745, value loss 0.02541, policy loss -0.05199\n",
      "Updates 12200, num timesteps 976080, FPS 188, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78744, value loss 0.01810, policy loss -0.00149\n",
      "Updates 12300, num timesteps 984080, FPS 188, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78400, value loss 0.05790, policy loss 0.09210\n",
      "Updates 12400, num timesteps 992080, FPS 187, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78817, value loss 0.01032, policy loss 0.05913\n",
      "Updates 12500, num timesteps 1000080, FPS 187, mean/median reward -19.7/-20.0, min/max reward -21.0/-16.0, entropy 1.78123, value loss 0.02798, policy loss -0.03757\n",
      "Updates 12600, num timesteps 1008080, FPS 187, mean/median reward -19.9/-20.0, min/max reward -21.0/-16.0, entropy 1.78815, value loss 0.02254, policy loss -0.04289\n",
      "Updates 12700, num timesteps 1016080, FPS 187, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78352, value loss 0.01171, policy loss -0.00086\n",
      "Updates 12800, num timesteps 1024080, FPS 187, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78680, value loss 0.04175, policy loss 0.06266\n",
      "Updates 12900, num timesteps 1032080, FPS 187, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78833, value loss 0.04560, policy loss 0.05780\n",
      "Updates 13000, num timesteps 1040080, FPS 187, mean/median reward -19.6/-20.0, min/max reward -21.0/-17.0, entropy 1.78740, value loss 0.00993, policy loss -0.02954\n",
      "Updates 13100, num timesteps 1048080, FPS 187, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78384, value loss 0.02969, policy loss 0.02908\n",
      "Updates 13200, num timesteps 1056080, FPS 186, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78528, value loss 0.01637, policy loss -0.01667\n",
      "Updates 13300, num timesteps 1064080, FPS 186, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.78627, value loss 0.01383, policy loss 0.00229\n",
      "Updates 13400, num timesteps 1072080, FPS 186, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78730, value loss 0.01569, policy loss -0.03467\n",
      "Updates 13500, num timesteps 1080080, FPS 186, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.78839, value loss 0.03122, policy loss -0.12557\n",
      "Updates 13600, num timesteps 1088080, FPS 186, mean/median reward -19.1/-20.0, min/max reward -21.0/-15.0, entropy 1.78791, value loss 0.05342, policy loss 0.04015\n",
      "Updates 13700, num timesteps 1096080, FPS 186, mean/median reward -19.6/-20.0, min/max reward -21.0/-15.0, entropy 1.78461, value loss 0.04231, policy loss 0.12448\n",
      "Updates 13800, num timesteps 1104080, FPS 186, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78539, value loss 0.00778, policy loss -0.05133\n",
      "Updates 13900, num timesteps 1112080, FPS 186, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.76871, value loss 0.01491, policy loss -0.01551\n",
      "Updates 14000, num timesteps 1120080, FPS 186, mean/median reward -19.2/-20.0, min/max reward -21.0/-16.0, entropy 1.77004, value loss 0.01253, policy loss 0.02110\n",
      "Updates 14100, num timesteps 1128080, FPS 186, mean/median reward -18.9/-20.0, min/max reward -21.0/-14.0, entropy 1.78416, value loss 0.01640, policy loss 0.12839\n",
      "Updates 14200, num timesteps 1136080, FPS 185, mean/median reward -18.9/-20.0, min/max reward -21.0/-14.0, entropy 1.78798, value loss 0.02826, policy loss 0.03898\n",
      "Updates 14300, num timesteps 1144080, FPS 185, mean/median reward -19.0/-19.0, min/max reward -21.0/-16.0, entropy 1.78124, value loss 0.06215, policy loss -0.14860\n",
      "Updates 14400, num timesteps 1152080, FPS 185, mean/median reward -19.1/-19.0, min/max reward -21.0/-16.0, entropy 1.78468, value loss 0.01987, policy loss 0.01347\n",
      "Updates 14500, num timesteps 1160080, FPS 185, mean/median reward -19.2/-19.0, min/max reward -21.0/-16.0, entropy 1.78629, value loss 0.02237, policy loss 0.00723\n",
      "Updates 14600, num timesteps 1168080, FPS 185, mean/median reward -19.5/-20.0, min/max reward -21.0/-17.0, entropy 1.78793, value loss 0.04413, policy loss -0.00208\n",
      "Updates 14700, num timesteps 1176080, FPS 185, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78562, value loss 0.02754, policy loss 0.07274\n",
      "Updates 14800, num timesteps 1184080, FPS 185, mean/median reward -19.4/-20.0, min/max reward -21.0/-16.0, entropy 1.77178, value loss 0.03243, policy loss -0.01236\n",
      "Updates 14900, num timesteps 1192080, FPS 185, mean/median reward -18.7/-19.0, min/max reward -21.0/-16.0, entropy 1.77702, value loss 0.02011, policy loss 0.02452\n",
      "Updates 15000, num timesteps 1200080, FPS 185, mean/median reward -18.8/-19.0, min/max reward -21.0/-17.0, entropy 1.77855, value loss 0.00450, policy loss -0.02413\n",
      "Updates 15100, num timesteps 1208080, FPS 185, mean/median reward -19.3/-19.0, min/max reward -21.0/-18.0, entropy 1.77867, value loss 0.03593, policy loss -0.04399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 15200, num timesteps 1216080, FPS 185, mean/median reward -19.1/-19.0, min/max reward -21.0/-16.0, entropy 1.78555, value loss 0.01491, policy loss -0.04333\n",
      "Updates 15300, num timesteps 1224080, FPS 185, mean/median reward -18.3/-18.0, min/max reward -20.0/-15.0, entropy 1.77793, value loss 0.01580, policy loss -0.03130\n",
      "Updates 15400, num timesteps 1232080, FPS 185, mean/median reward -18.2/-18.0, min/max reward -20.0/-15.0, entropy 1.78062, value loss 0.02618, policy loss 0.04495\n",
      "Updates 15500, num timesteps 1240080, FPS 185, mean/median reward -18.5/-19.0, min/max reward -21.0/-15.0, entropy 1.78447, value loss 0.00849, policy loss 0.07278\n",
      "Updates 15600, num timesteps 1248080, FPS 185, mean/median reward -19.0/-20.0, min/max reward -21.0/-16.0, entropy 1.75141, value loss 0.03649, policy loss -0.07433\n",
      "Updates 15700, num timesteps 1256080, FPS 185, mean/median reward -19.1/-20.0, min/max reward -21.0/-16.0, entropy 1.76580, value loss 0.02458, policy loss -0.00266\n",
      "Updates 15800, num timesteps 1264080, FPS 185, mean/median reward -18.8/-19.0, min/max reward -21.0/-16.0, entropy 1.77248, value loss 0.01394, policy loss 0.00100\n",
      "Updates 15900, num timesteps 1272080, FPS 185, mean/median reward -18.6/-19.0, min/max reward -21.0/-16.0, entropy 1.77797, value loss 0.03209, policy loss -0.06260\n",
      "Updates 16000, num timesteps 1280080, FPS 185, mean/median reward -18.9/-19.0, min/max reward -21.0/-16.0, entropy 1.77598, value loss 0.02409, policy loss 0.00612\n",
      "Updates 16100, num timesteps 1288080, FPS 185, mean/median reward -18.6/-19.0, min/max reward -21.0/-15.0, entropy 1.78368, value loss 0.03633, policy loss -0.11149\n",
      "Updates 16200, num timesteps 1296080, FPS 185, mean/median reward -18.2/-18.0, min/max reward -21.0/-15.0, entropy 1.77908, value loss 0.02639, policy loss -0.08942\n",
      "Updates 16300, num timesteps 1304080, FPS 185, mean/median reward -18.2/-18.0, min/max reward -21.0/-15.0, entropy 1.77034, value loss 0.05564, policy loss 0.07911\n",
      "Updates 16400, num timesteps 1312080, FPS 185, mean/median reward -18.4/-18.0, min/max reward -21.0/-16.0, entropy 1.75044, value loss 0.01374, policy loss 0.04030\n",
      "Updates 16500, num timesteps 1320080, FPS 185, mean/median reward -18.6/-18.0, min/max reward -21.0/-17.0, entropy 1.76058, value loss 0.05844, policy loss 0.00614\n",
      "Updates 16600, num timesteps 1328080, FPS 184, mean/median reward -18.5/-18.0, min/max reward -21.0/-15.0, entropy 1.75542, value loss 0.01455, policy loss -0.02934\n",
      "Updates 16700, num timesteps 1336080, FPS 183, mean/median reward -18.2/-18.0, min/max reward -21.0/-14.0, entropy 1.78246, value loss 0.00930, policy loss -0.06938\n",
      "Updates 16800, num timesteps 1344080, FPS 183, mean/median reward -18.2/-19.0, min/max reward -21.0/-14.0, entropy 1.78220, value loss 0.01589, policy loss 0.02407\n",
      "Updates 16900, num timesteps 1352080, FPS 183, mean/median reward -18.1/-18.0, min/max reward -21.0/-15.0, entropy 1.78371, value loss 0.01790, policy loss -0.00206\n",
      "Updates 17000, num timesteps 1360080, FPS 183, mean/median reward -17.5/-18.0, min/max reward -21.0/-13.0, entropy 1.77554, value loss 0.01702, policy loss 0.00505\n",
      "Updates 17100, num timesteps 1368080, FPS 183, mean/median reward -17.2/-18.0, min/max reward -20.0/-13.0, entropy 1.77575, value loss 0.01323, policy loss -0.03040\n",
      "Updates 17200, num timesteps 1376080, FPS 183, mean/median reward -18.1/-19.0, min/max reward -20.0/-14.0, entropy 1.76889, value loss 0.02609, policy loss -0.02198\n",
      "Updates 17300, num timesteps 1384080, FPS 183, mean/median reward -18.2/-19.0, min/max reward -20.0/-15.0, entropy 1.78370, value loss 0.01167, policy loss 0.00110\n",
      "Updates 17400, num timesteps 1392080, FPS 183, mean/median reward -18.3/-19.0, min/max reward -21.0/-15.0, entropy 1.78379, value loss 0.05047, policy loss 0.11660\n",
      "Updates 17500, num timesteps 1400080, FPS 183, mean/median reward -18.1/-18.0, min/max reward -21.0/-15.0, entropy 1.78317, value loss 0.01022, policy loss -0.02025\n",
      "Updates 17600, num timesteps 1408080, FPS 183, mean/median reward -18.4/-19.0, min/max reward -21.0/-15.0, entropy 1.77523, value loss 0.04519, policy loss 0.02807\n",
      "Updates 17700, num timesteps 1416080, FPS 183, mean/median reward -18.0/-18.0, min/max reward -21.0/-13.0, entropy 1.78028, value loss 0.01952, policy loss 0.09264\n",
      "Updates 17800, num timesteps 1424080, FPS 183, mean/median reward -17.5/-18.0, min/max reward -21.0/-13.0, entropy 1.76855, value loss 0.00797, policy loss -0.02333\n",
      "Updates 17900, num timesteps 1432080, FPS 182, mean/median reward -17.5/-18.0, min/max reward -21.0/-13.0, entropy 1.75341, value loss 0.01708, policy loss 0.01664\n",
      "Updates 18000, num timesteps 1440080, FPS 182, mean/median reward -17.9/-18.0, min/max reward -19.0/-16.0, entropy 1.77075, value loss 0.00823, policy loss -0.00501\n",
      "Updates 18100, num timesteps 1448080, FPS 181, mean/median reward -18.2/-19.0, min/max reward -19.0/-16.0, entropy 1.76698, value loss 0.01004, policy loss 0.00915\n",
      "Updates 18200, num timesteps 1456080, FPS 181, mean/median reward -18.0/-18.0, min/max reward -20.0/-16.0, entropy 1.74650, value loss 0.03329, policy loss 0.07660\n",
      "Updates 18300, num timesteps 1464080, FPS 181, mean/median reward -17.8/-19.0, min/max reward -20.0/-14.0, entropy 1.76561, value loss 0.03001, policy loss -0.12838\n",
      "Updates 18400, num timesteps 1472080, FPS 181, mean/median reward -17.9/-19.0, min/max reward -20.0/-14.0, entropy 1.77755, value loss 0.02488, policy loss 0.01062\n",
      "Updates 18500, num timesteps 1480080, FPS 181, mean/median reward -18.1/-19.0, min/max reward -20.0/-15.0, entropy 1.76943, value loss 0.04263, policy loss -0.01485\n",
      "Updates 18600, num timesteps 1488080, FPS 181, mean/median reward -17.1/-17.0, min/max reward -20.0/-13.0, entropy 1.78230, value loss 0.01098, policy loss 0.03997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-78:\n",
      "Process Process-72:\n",
      "Process Process-66:\n",
      "Process Process-76:\n",
      "Process Process-79:\n",
      "Process Process-71:\n",
      "Process Process-74:\n",
      "Process Process-80:\n",
      "Process Process-73:\n",
      "Process Process-68:\n",
      "Process Process-67:\n",
      "Process Process-70:\n",
      "Process Process-75:\n",
      "Process Process-77:\n",
      "Process Process-69:\n",
      "Process Process-65:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-7-81cd713ae1b0>\", line 7, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e0d8cdf4ece3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m                        value_loss.data[0], action_loss.data[0]))\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e0d8cdf4ece3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m                                                                                        \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                                                        \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                                                                                        Variable(rollouts.actions.view(-1, action_shape)))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8695ba22371b>\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, inputs, states, masks, actions)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0maction_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprobs_and_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8695ba22371b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, states, masks)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    envs = [make_env(args.env_name, args.seed, i, args.log_dir)\n",
    "                for i in range(args.num_processes)]\n",
    "\n",
    "    if args.num_processes > 1:\n",
    "        envs = SubprocVecEnv(envs)\n",
    "    else:\n",
    "        envs = DummyVecEnv(envs)\n",
    "\n",
    "    obs_shape = envs.observation_space.shape\n",
    "    obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "\n",
    "    global actor_critic\n",
    "    \n",
    "    actor_critic = CNNPolicy(obs_shape[0], envs.action_space)\n",
    "  \n",
    "    if args.load_model:\n",
    "        actor_critic.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "    action_shape = 1\n",
    "\n",
    "    if args.cuda:\n",
    "        actor_critic.cuda()\n",
    "\n",
    "    optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space,\\\n",
    "                              actor_critic.state_size)\n",
    "    \n",
    "    current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "    def update_current_obs(obs):\n",
    "        shape_dim0 = envs.observation_space.shape[0]\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        if args.num_stack > 1:\n",
    "            current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "    obs = envs.reset()\n",
    "    update_current_obs(obs)\n",
    "\n",
    "    rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "    # These variables are used to compute average rewards for all processes.\n",
    "    episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "    final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "    if args.cuda:\n",
    "        current_obs = current_obs.cuda()\n",
    "        rollouts.cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(args.num_steps):\n",
    "            # Sample actions\n",
    "            value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                      Variable(rollouts.states[step], volatile=True),\n",
    "                                                                      Variable(rollouts.masks[step], volatile=True))\n",
    "            cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "            episode_rewards += reward\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            final_rewards *= masks\n",
    "            final_rewards += (1 - masks) * episode_rewards\n",
    "            episode_rewards *= masks\n",
    "\n",
    "            if args.cuda:\n",
    "                masks = masks.cuda()\n",
    "\n",
    "            if current_obs.dim() == 4:\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "            else:\n",
    "                current_obs *= masks\n",
    "\n",
    "            update_current_obs(obs)\n",
    "            rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks)\n",
    "\n",
    "        next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                                  Variable(rollouts.states[-1], volatile=True),\n",
    "                                  Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "        values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                       Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                       Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                       Variable(rollouts.actions.view(-1, action_shape)))\n",
    "\n",
    "        values = values.view(args.num_steps, args.num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "\n",
    "        advantages = Variable(rollouts.returns[:-1]) - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef).backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    " \n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % args.save_interval == 0 and args.save_model:\n",
    "            \n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            # A really ugly way to save a model to CPU\n",
    "            save_model = actor_critic\n",
    "            if args.cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "                \n",
    "            torch.save(actor_critic.state_dict(), SAVE_PATH)\n",
    "\n",
    "        if j % args.log_interval == 0:\n",
    "            end = time.time()\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "                format(j, total_num_steps,\n",
    "                       int(total_num_steps / (end - start)),\n",
    "                       final_rewards.mean(),\n",
    "                       final_rewards.median(),\n",
    "                       final_rewards.min(),\n",
    "                       final_rewards.max(), dist_entropy.data[0],\n",
    "                       value_loss.data[0], action_loss.data[0]))\n",
    "            print(\"\\n rollout returns\", rollouts.returns, \"\\nvalues\", values)\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import orthogonal\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        x = self(x)\n",
    "\n",
    "        probs = F.softmax(x)\n",
    "        if deterministic is False:\n",
    "            action = probs.multinomial()\n",
    "        else:\n",
    "            action = probs.max(1)[1]\n",
    "        return action\n",
    "\n",
    "    def logprobs_and_entropy(self, x, actions):\n",
    "        x = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(x)\n",
    "        probs = F.softmax(x)\n",
    "\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "\n",
    "        dist_entropy = -(log_probs * probs).sum(-1).mean()\n",
    "        return action_log_probs, dist_entropy\n",
    "    \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        orthogonal(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(32 * 7 * 7, 512)\n",
    "\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "\n",
    "        num_outputs = action_space.n\n",
    "        self.dist = Categorical(512, num_outputs)\n",
    "\n",
    "        self.train() # training mode. Only affects dropout, batchnorm etc\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def act(self, inputs, states, masks, deterministic=False):\n",
    "        value, x, states = self(inputs, states, masks)\n",
    "        action = self.dist.sample(x, deterministic=deterministic)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, action)\n",
    "        return value, action, action_log_probs, states\n",
    "\n",
    "    def evaluate_actions(self, inputs, states, masks, actions):\n",
    "        value, x, states = self(inputs, states, masks)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, actions)\n",
    "        return value, action_log_probs, dist_entropy, states\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 1\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "        relu_gain = nn.init.calculate_gain('relu')\n",
    "        self.conv1.weight.data.mul_(relu_gain)\n",
    "        self.conv2.weight.data.mul_(relu_gain)\n",
    "        self.conv3.weight.data.mul_(relu_gain)\n",
    "        self.linear1.weight.data.mul_(relu_gain)\n",
    "\n",
    "    def forward(self, inputs, states, masks):\n",
    "        x = self.conv1(inputs / 255.0)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.critic_linear(x), x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space, state_size):\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.states = torch.zeros(num_steps + 1, num_processes, state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        \n",
    "        action_shape = 1\n",
    "\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "            \n",
    "        self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.observations = self.observations.cuda()\n",
    "        self.states = self.states.cuda()\n",
    "        self.rewards = self.rewards.cuda()\n",
    "        self.value_preds = self.value_preds.cuda()\n",
    "        self.returns = self.returns.cuda()\n",
    "        self.action_log_probs = self.action_log_probs.cuda()\n",
    "        self.actions = self.actions.cuda()\n",
    "        self.masks = self.masks.cuda()\n",
    "\n",
    "    def insert(self, step, current_obs, state, action, action_log_prob, value_pred, reward, mask):\n",
    "        self.observations[step + 1].copy_(current_obs)\n",
    "        self.states[step + 1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.action_log_probs[step].copy_(action_log_prob)\n",
    "        self.value_preds[step].copy_(value_pred)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step + 1].copy_(mask)\n",
    "\n",
    "    def after_update(self):\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, use_gae, gamma, tau):\n",
    "        self.returns[-1] = next_value\n",
    "        for step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[step] = self.returns[step + 1] * \\\n",
    "                gamma * self.masks[step + 1] + self.rewards[step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x()\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.action_space, env.observation_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "\n",
    "\n",
    "class SubprocVecEnv(object):\n",
    "    def __init__(self, env_fns):\n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        \"\"\"\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        self.action_space, self.observation_space = self.remotes[0].recv()\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "        self.closed = True\n",
    "\n",
    "    @property\n",
    "    def num_envs(self):\n",
    "        return len(self.remotes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
