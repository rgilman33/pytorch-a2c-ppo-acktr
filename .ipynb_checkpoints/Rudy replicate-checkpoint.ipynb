{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py:680: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  not cbook.is_string_like(colors[0]):\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from arguments import get_args\n",
    "\n",
    "#from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "#from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "#from baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "from all_stuff import * # this has the above modules consolidated into a single file. god this was a bitch\n",
    "\n",
    "from envs import make_env # had to manually add some files into directory for env to reference bc baselines \n",
    "# modules not working right\n",
    "\n",
    "from kfac import KFACOptimizer\n",
    "from model import CNNPolicy, MLPPolicy\n",
    "from storage import RolloutStorage\n",
    "from visualize import visdom_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=True\n",
    "        self.algo='a2c'\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=16\n",
    "        self.recurrent_policy=False\n",
    "        self.vis=False\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=3e6\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.vis_interval=100\n",
    "        \n",
    "args = args()\n",
    "\n",
    "save_path = os.path.join(args.save_dir, args.algo)\n",
    "SAVE_PATH = os.path.join(save_path, args.env_name + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######\n",
      "WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\n",
      "#######\n",
      "loading saved model from  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 0, num timesteps 80, FPS 157, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.03279, value loss 0.88986, policy loss 0.44509\n",
      "Updates 100, num timesteps 8080, FPS 193, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.76336, value loss 0.03482, policy loss 0.05052\n",
      "Updates 200, num timesteps 16080, FPS 199, mean/median reward -12.9/-20.0, min/max reward -21.0/0.0, entropy 1.78250, value loss 0.03573, policy loss 0.06320\n",
      "Updates 300, num timesteps 24080, FPS 199, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78009, value loss 0.04597, policy loss 0.06223\n",
      "Updates 400, num timesteps 32080, FPS 200, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78890, value loss 0.00247, policy loss 0.08026\n",
      "Updates 500, num timesteps 40080, FPS 199, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.78891, value loss 0.00437, policy loss 0.10655\n",
      "Updates 600, num timesteps 48080, FPS 198, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78721, value loss 0.00685, policy loss 0.13266\n",
      "Updates 700, num timesteps 56080, FPS 197, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78759, value loss 0.11307, policy loss -0.17230\n",
      "Updates 800, num timesteps 64080, FPS 196, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78828, value loss 0.01456, policy loss 0.08743\n",
      "Updates 900, num timesteps 72080, FPS 193, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78751, value loss 0.04702, policy loss 0.00771\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 1000, num timesteps 80080, FPS 191, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.76984, value loss 0.06707, policy loss 0.13684\n",
      "Updates 1100, num timesteps 88080, FPS 189, mean/median reward -20.2/-20.0, min/max reward -21.0/-17.0, entropy 1.79095, value loss 0.02354, policy loss 0.04113\n",
      "Updates 1200, num timesteps 96080, FPS 187, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78149, value loss 0.03562, policy loss -0.02808\n",
      "Updates 1300, num timesteps 104080, FPS 185, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78761, value loss 0.04450, policy loss -0.01046\n",
      "Updates 1400, num timesteps 112080, FPS 184, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79043, value loss 0.01899, policy loss 0.10771\n",
      "Updates 1500, num timesteps 120080, FPS 184, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79057, value loss 0.00405, policy loss 0.02098\n",
      "Updates 1600, num timesteps 128080, FPS 178, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78924, value loss 0.01647, policy loss 0.07389\n",
      "Updates 1700, num timesteps 136080, FPS 174, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.78172, value loss 0.03268, policy loss 0.08109\n",
      "Updates 1800, num timesteps 144080, FPS 174, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.78406, value loss 0.04298, policy loss -0.01885\n",
      "Updates 1900, num timesteps 152080, FPS 175, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78534, value loss 0.04832, policy loss -0.02658\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 2000, num timesteps 160080, FPS 174, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.79043, value loss 0.07652, policy loss -0.09294\n",
      "Updates 2100, num timesteps 168080, FPS 174, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78955, value loss 0.10179, policy loss 0.11959\n",
      "Updates 2200, num timesteps 176080, FPS 173, mean/median reward -20.6/-21.0, min/max reward -21.0/-18.0, entropy 1.78925, value loss 0.02160, policy loss -0.04546\n",
      "Updates 2300, num timesteps 184080, FPS 171, mean/median reward -20.5/-21.0, min/max reward -21.0/-18.0, entropy 1.78253, value loss 0.02867, policy loss -0.00224\n",
      "Updates 2400, num timesteps 192080, FPS 170, mean/median reward -20.5/-21.0, min/max reward -21.0/-18.0, entropy 1.79091, value loss 0.00974, policy loss 0.01924\n",
      "Updates 2500, num timesteps 200080, FPS 170, mean/median reward -20.6/-21.0, min/max reward -21.0/-20.0, entropy 1.79084, value loss 0.01919, policy loss -0.06105\n",
      "Updates 2600, num timesteps 208080, FPS 169, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.78989, value loss 0.05800, policy loss -0.10071\n",
      "Updates 2700, num timesteps 216080, FPS 169, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78797, value loss 0.07204, policy loss 0.02752\n",
      "Updates 2800, num timesteps 224080, FPS 168, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.79080, value loss 0.03104, policy loss -0.10408\n",
      "Updates 2900, num timesteps 232080, FPS 167, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78967, value loss 0.02613, policy loss 0.05151\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 3000, num timesteps 240080, FPS 165, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78715, value loss 0.02478, policy loss -0.03149\n",
      "Updates 3100, num timesteps 248080, FPS 164, mean/median reward -20.0/-20.0, min/max reward -21.0/-19.0, entropy 1.78833, value loss 0.02900, policy loss 0.02218\n",
      "Updates 3200, num timesteps 256080, FPS 162, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.78914, value loss 0.00934, policy loss 0.01363\n",
      "Updates 3300, num timesteps 264080, FPS 162, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79091, value loss 0.04807, policy loss -0.01851\n",
      "Updates 3400, num timesteps 272080, FPS 161, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.79086, value loss 0.00865, policy loss -0.03475\n",
      "Updates 3500, num timesteps 280080, FPS 161, mean/median reward -19.8/-20.0, min/max reward -21.0/-19.0, entropy 1.78740, value loss 0.02174, policy loss 0.02254\n",
      "Updates 3600, num timesteps 288080, FPS 160, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.79055, value loss 0.00921, policy loss 0.03068\n",
      "Updates 3700, num timesteps 296080, FPS 160, mean/median reward -20.2/-21.0, min/max reward -21.0/-19.0, entropy 1.78853, value loss 0.01790, policy loss 0.03945\n",
      "Updates 3800, num timesteps 304080, FPS 160, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.79014, value loss 0.01402, policy loss -0.06414\n",
      "Updates 3900, num timesteps 312080, FPS 159, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.79029, value loss 0.03037, policy loss -0.00925\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 4000, num timesteps 320080, FPS 159, mean/median reward -20.0/-20.0, min/max reward -21.0/-17.0, entropy 1.78783, value loss 0.01478, policy loss 0.05306\n",
      "Updates 4100, num timesteps 328080, FPS 159, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.79087, value loss 0.02470, policy loss -0.08355\n",
      "Updates 4200, num timesteps 336080, FPS 158, mean/median reward -20.0/-20.0, min/max reward -21.0/-17.0, entropy 1.79004, value loss 0.02873, policy loss 0.00691\n",
      "Updates 4300, num timesteps 344080, FPS 158, mean/median reward -20.4/-20.0, min/max reward -21.0/-19.0, entropy 1.79101, value loss 0.09484, policy loss -0.03960\n",
      "Updates 4400, num timesteps 352080, FPS 158, mean/median reward -20.3/-21.0, min/max reward -21.0/-17.0, entropy 1.79039, value loss 0.03059, policy loss 0.01280\n",
      "Updates 4500, num timesteps 360080, FPS 158, mean/median reward -20.2/-21.0, min/max reward -21.0/-17.0, entropy 1.79040, value loss 0.03565, policy loss 0.13078\n",
      "Updates 4600, num timesteps 368080, FPS 157, mean/median reward -20.6/-21.0, min/max reward -21.0/-19.0, entropy 1.79139, value loss 0.06778, policy loss -0.05363\n",
      "Updates 4700, num timesteps 376080, FPS 157, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79044, value loss 0.02348, policy loss 0.01484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 4800, num timesteps 384080, FPS 157, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.79076, value loss 0.01469, policy loss 0.05062\n",
      "Updates 4900, num timesteps 392080, FPS 157, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79084, value loss 0.01497, policy loss -0.01229\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 5000, num timesteps 400080, FPS 157, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78908, value loss 0.05311, policy loss 0.09311\n",
      "Updates 5100, num timesteps 408080, FPS 156, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78921, value loss 0.00977, policy loss 0.04849\n",
      "Updates 5200, num timesteps 416080, FPS 156, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79086, value loss 0.03339, policy loss 0.08501\n",
      "Updates 5300, num timesteps 424080, FPS 156, mean/median reward -20.5/-21.0, min/max reward -21.0/-19.0, entropy 1.79129, value loss 0.02372, policy loss -0.09945\n",
      "Updates 5400, num timesteps 432080, FPS 156, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79099, value loss 0.03466, policy loss 0.04874\n",
      "Updates 5500, num timesteps 440080, FPS 156, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79115, value loss 0.06355, policy loss 0.07423\n",
      "Updates 5600, num timesteps 448080, FPS 156, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79028, value loss 0.05629, policy loss 0.10514\n",
      "Updates 5700, num timesteps 456080, FPS 155, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78940, value loss 0.02461, policy loss 0.02224\n",
      "Updates 5800, num timesteps 464080, FPS 155, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79064, value loss 0.03649, policy loss 0.06014\n",
      "Updates 5900, num timesteps 472080, FPS 155, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.79133, value loss 0.02058, policy loss -0.02334\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 6000, num timesteps 480080, FPS 155, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.78839, value loss 0.02740, policy loss 0.09745\n",
      "Updates 6100, num timesteps 488080, FPS 155, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78870, value loss 0.02826, policy loss -0.01377\n",
      "Updates 6200, num timesteps 496080, FPS 154, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79071, value loss 0.03097, policy loss 0.01463\n",
      "Updates 6300, num timesteps 504080, FPS 154, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78982, value loss 0.01477, policy loss -0.06318\n",
      "Updates 6400, num timesteps 512080, FPS 154, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.79117, value loss 0.09272, policy loss 0.10465\n",
      "Updates 6500, num timesteps 520080, FPS 154, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78944, value loss 0.04122, policy loss -0.07992\n",
      "Updates 6600, num timesteps 528080, FPS 154, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.79116, value loss 0.01159, policy loss 0.03319\n",
      "Updates 6700, num timesteps 536080, FPS 153, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78974, value loss 0.01004, policy loss 0.02107\n",
      "Updates 6800, num timesteps 544080, FPS 153, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78887, value loss 0.00974, policy loss -0.00009\n",
      "Updates 6900, num timesteps 552080, FPS 153, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.79140, value loss 0.01922, policy loss -0.04529\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 7000, num timesteps 560080, FPS 153, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78684, value loss 0.01405, policy loss -0.01149\n",
      "Updates 7100, num timesteps 568080, FPS 153, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79141, value loss 0.01354, policy loss -0.05426\n",
      "Updates 7200, num timesteps 576080, FPS 153, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.79094, value loss 0.01647, policy loss 0.03756\n",
      "Updates 7300, num timesteps 584080, FPS 152, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.79069, value loss 0.01446, policy loss 0.03403\n",
      "Updates 7400, num timesteps 592080, FPS 152, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78817, value loss 0.08097, policy loss -0.21638\n",
      "Updates 7500, num timesteps 600080, FPS 152, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78900, value loss 0.01225, policy loss 0.03424\n",
      "Updates 7600, num timesteps 608080, FPS 152, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78642, value loss 0.03415, policy loss 0.01976\n",
      "Updates 7700, num timesteps 616080, FPS 152, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78983, value loss 0.02702, policy loss 0.00689\n",
      "Updates 7800, num timesteps 624080, FPS 152, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78665, value loss 0.01739, policy loss -0.00995\n",
      "Updates 7900, num timesteps 632080, FPS 152, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78588, value loss 0.01665, policy loss -0.05193\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 8000, num timesteps 640080, FPS 152, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.79031, value loss 0.02413, policy loss -0.01383\n",
      "Updates 8100, num timesteps 648080, FPS 151, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.79020, value loss 0.04792, policy loss 0.04721\n",
      "Updates 8200, num timesteps 656080, FPS 151, mean/median reward -20.1/-21.0, min/max reward -21.0/-18.0, entropy 1.79119, value loss 0.02357, policy loss -0.00290\n",
      "Updates 8300, num timesteps 664080, FPS 151, mean/median reward -20.2/-20.0, min/max reward -21.0/-19.0, entropy 1.78690, value loss 0.01830, policy loss 0.06203\n",
      "Updates 8400, num timesteps 672080, FPS 151, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.79008, value loss 0.01386, policy loss 0.01307\n",
      "Updates 8500, num timesteps 680080, FPS 151, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.79081, value loss 0.02768, policy loss -0.08980\n",
      "Updates 8600, num timesteps 688080, FPS 151, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.79121, value loss 0.01840, policy loss -0.03303\n",
      "Updates 8700, num timesteps 696080, FPS 151, mean/median reward -20.2/-21.0, min/max reward -21.0/-19.0, entropy 1.78951, value loss 0.01519, policy loss 0.03144\n",
      "Updates 8800, num timesteps 704080, FPS 151, mean/median reward -20.2/-21.0, min/max reward -21.0/-19.0, entropy 1.78979, value loss 0.00975, policy loss -0.02044\n",
      "Updates 8900, num timesteps 712080, FPS 151, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.79056, value loss 0.04141, policy loss -0.04740\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 9000, num timesteps 720080, FPS 151, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.79093, value loss 0.01987, policy loss -0.02205\n",
      "Updates 9100, num timesteps 728080, FPS 151, mean/median reward -19.9/-20.0, min/max reward -21.0/-19.0, entropy 1.78974, value loss 0.03447, policy loss -0.00028\n",
      "Updates 9200, num timesteps 736080, FPS 151, mean/median reward -19.8/-20.0, min/max reward -21.0/-16.0, entropy 1.79082, value loss 0.02657, policy loss 0.00071\n",
      "Updates 9300, num timesteps 744080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-16.0, entropy 1.78913, value loss 0.01819, policy loss 0.00290\n",
      "Updates 9400, num timesteps 752080, FPS 150, mean/median reward -20.6/-21.0, min/max reward -21.0/-20.0, entropy 1.79077, value loss 0.02368, policy loss 0.01299\n",
      "Updates 9500, num timesteps 760080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78941, value loss 0.03267, policy loss -0.08840\n",
      "Updates 9600, num timesteps 768080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78989, value loss 0.02066, policy loss -0.14804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9700, num timesteps 776080, FPS 150, mean/median reward -20.0/-20.0, min/max reward -21.0/-19.0, entropy 1.78875, value loss 0.02652, policy loss -0.03395\n",
      "Updates 9800, num timesteps 784080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78925, value loss 0.00621, policy loss 0.02284\n",
      "Updates 9900, num timesteps 792080, FPS 150, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.79028, value loss 0.02032, policy loss 0.01185\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 10000, num timesteps 800080, FPS 150, mean/median reward -20.3/-21.0, min/max reward -21.0/-18.0, entropy 1.78796, value loss 0.03237, policy loss 0.01538\n",
      "Updates 10100, num timesteps 808080, FPS 150, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78824, value loss 0.01167, policy loss -0.01512\n",
      "Updates 10200, num timesteps 816080, FPS 150, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78794, value loss 0.01797, policy loss 0.07293\n",
      "Updates 10300, num timesteps 824080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78527, value loss 0.01588, policy loss -0.05738\n",
      "Updates 10400, num timesteps 832080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78994, value loss 0.01243, policy loss -0.04768\n",
      "Updates 10500, num timesteps 840080, FPS 150, mean/median reward -20.3/-20.0, min/max reward -21.0/-19.0, entropy 1.79043, value loss 0.00998, policy loss 0.01466\n",
      "Updates 10600, num timesteps 848080, FPS 150, mean/median reward -20.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78745, value loss 0.01663, policy loss 0.05366\n",
      "Updates 10700, num timesteps 856080, FPS 150, mean/median reward -20.1/-20.0, min/max reward -21.0/-17.0, entropy 1.79065, value loss 0.00426, policy loss 0.03132\n",
      "Updates 10800, num timesteps 864080, FPS 150, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78913, value loss 0.01712, policy loss -0.01893\n",
      "Updates 10900, num timesteps 872080, FPS 150, mean/median reward -19.5/-20.0, min/max reward -21.0/-17.0, entropy 1.78728, value loss 0.02075, policy loss 0.03679\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 11000, num timesteps 880080, FPS 150, mean/median reward -19.8/-20.0, min/max reward -21.0/-17.0, entropy 1.78838, value loss 0.01367, policy loss -0.05286\n",
      "Updates 11100, num timesteps 888080, FPS 150, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78923, value loss 0.02486, policy loss 0.05005\n",
      "Updates 11200, num timesteps 896080, FPS 150, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78818, value loss 0.04278, policy loss 0.03801\n",
      "Updates 11300, num timesteps 904080, FPS 150, mean/median reward -19.8/-20.0, min/max reward -21.0/-19.0, entropy 1.78801, value loss 0.01745, policy loss 0.02108\n",
      "Updates 11400, num timesteps 912080, FPS 149, mean/median reward -20.1/-20.0, min/max reward -21.0/-19.0, entropy 1.78982, value loss 0.04264, policy loss 0.04069\n",
      "Updates 11500, num timesteps 920080, FPS 149, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78913, value loss 0.01233, policy loss -0.01019\n",
      "Updates 11600, num timesteps 928080, FPS 149, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78996, value loss 0.00505, policy loss -0.02035\n",
      "Updates 11700, num timesteps 936080, FPS 149, mean/median reward -20.2/-20.0, min/max reward -21.0/-18.0, entropy 1.78448, value loss 0.04682, policy loss -0.06002\n",
      "Updates 11800, num timesteps 944080, FPS 149, mean/median reward -20.3/-21.0, min/max reward -21.0/-17.0, entropy 1.78785, value loss 0.04980, policy loss 0.08781\n",
      "Updates 11900, num timesteps 952080, FPS 149, mean/median reward -20.4/-21.0, min/max reward -21.0/-18.0, entropy 1.78893, value loss 0.01002, policy loss 0.02165\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 12000, num timesteps 960080, FPS 149, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78953, value loss 0.00759, policy loss 0.00637\n",
      "Updates 12100, num timesteps 968080, FPS 149, mean/median reward -20.3/-21.0, min/max reward -21.0/-19.0, entropy 1.78746, value loss 0.02034, policy loss -0.02810\n",
      "Updates 12200, num timesteps 976080, FPS 149, mean/median reward -20.4/-21.0, min/max reward -21.0/-19.0, entropy 1.78942, value loss 0.01092, policy loss 0.01600\n",
      "Updates 12300, num timesteps 984080, FPS 149, mean/median reward -20.2/-21.0, min/max reward -21.0/-18.0, entropy 1.78765, value loss 0.01263, policy loss -0.04446\n",
      "Updates 12400, num timesteps 992080, FPS 149, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78877, value loss 0.03765, policy loss 0.05658\n",
      "Updates 12500, num timesteps 1000080, FPS 149, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78669, value loss 0.01185, policy loss -0.02964\n",
      "Updates 12600, num timesteps 1008080, FPS 149, mean/median reward -19.9/-20.0, min/max reward -21.0/-19.0, entropy 1.78872, value loss 0.02998, policy loss -0.00205\n",
      "Updates 12700, num timesteps 1016080, FPS 149, mean/median reward -20.1/-20.0, min/max reward -21.0/-18.0, entropy 1.78605, value loss 0.01380, policy loss -0.05963\n",
      "Updates 12800, num timesteps 1024080, FPS 149, mean/median reward -20.3/-20.0, min/max reward -21.0/-18.0, entropy 1.79014, value loss 0.01123, policy loss -0.01132\n",
      "Updates 12900, num timesteps 1032080, FPS 148, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78826, value loss 0.02918, policy loss -0.07635\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 13000, num timesteps 1040080, FPS 149, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78792, value loss 0.01045, policy loss -0.01201\n",
      "Updates 13100, num timesteps 1048080, FPS 149, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78897, value loss 0.00536, policy loss -0.00731\n",
      "Updates 13200, num timesteps 1056080, FPS 149, mean/median reward -20.0/-20.0, min/max reward -21.0/-18.0, entropy 1.78896, value loss 0.02362, policy loss 0.02301\n",
      "Updates 13300, num timesteps 1064080, FPS 149, mean/median reward -19.7/-20.0, min/max reward -21.0/-18.0, entropy 1.78428, value loss 0.01907, policy loss -0.04544\n",
      "Updates 13400, num timesteps 1072080, FPS 149, mean/median reward -19.8/-20.0, min/max reward -21.0/-19.0, entropy 1.78284, value loss 0.01500, policy loss 0.04804\n",
      "Updates 13500, num timesteps 1080080, FPS 149, mean/median reward -19.5/-19.0, min/max reward -21.0/-18.0, entropy 1.78418, value loss 0.06159, policy loss 0.00885\n",
      "Updates 13600, num timesteps 1088080, FPS 149, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78456, value loss 0.04823, policy loss -0.09551\n",
      "Updates 13700, num timesteps 1096080, FPS 148, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.78360, value loss 0.02665, policy loss 0.06438\n",
      "Updates 13800, num timesteps 1104080, FPS 148, mean/median reward -19.1/-20.0, min/max reward -21.0/-17.0, entropy 1.78727, value loss 0.01742, policy loss 0.02345\n",
      "Updates 13900, num timesteps 1112080, FPS 148, mean/median reward -19.1/-19.0, min/max reward -21.0/-17.0, entropy 1.78913, value loss 0.04321, policy loss -0.02816\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 14000, num timesteps 1120080, FPS 148, mean/median reward -19.4/-19.0, min/max reward -21.0/-17.0, entropy 1.78205, value loss 0.03825, policy loss -0.08174\n",
      "Updates 14100, num timesteps 1128080, FPS 148, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.78554, value loss 0.01577, policy loss 0.01435\n",
      "Updates 14200, num timesteps 1136080, FPS 148, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78909, value loss 0.03572, policy loss 0.10043\n",
      "Updates 14300, num timesteps 1144080, FPS 148, mean/median reward -19.8/-20.0, min/max reward -21.0/-18.0, entropy 1.77701, value loss 0.00897, policy loss -0.00520\n",
      "Updates 14400, num timesteps 1152080, FPS 148, mean/median reward -19.9/-20.0, min/max reward -21.0/-17.0, entropy 1.78126, value loss 0.01099, policy loss 0.02717\n",
      "Updates 14500, num timesteps 1160080, FPS 148, mean/median reward -20.1/-21.0, min/max reward -21.0/-17.0, entropy 1.78419, value loss 0.03803, policy loss 0.11292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 14600, num timesteps 1168080, FPS 148, mean/median reward -19.9/-20.0, min/max reward -21.0/-18.0, entropy 1.78532, value loss 0.04796, policy loss 0.04365\n",
      "Updates 14700, num timesteps 1176080, FPS 148, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.78580, value loss 0.04576, policy loss 0.09878\n",
      "Updates 14800, num timesteps 1184080, FPS 148, mean/median reward -19.6/-20.0, min/max reward -21.0/-18.0, entropy 1.77884, value loss 0.03025, policy loss 0.06919\n",
      "Updates 14900, num timesteps 1192080, FPS 148, mean/median reward -19.5/-20.0, min/max reward -21.0/-17.0, entropy 1.77817, value loss 0.01886, policy loss -0.01779\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 15000, num timesteps 1200080, FPS 148, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.78200, value loss 0.01004, policy loss 0.03074\n",
      "Updates 15100, num timesteps 1208080, FPS 148, mean/median reward -19.2/-19.0, min/max reward -21.0/-17.0, entropy 1.78525, value loss 0.01169, policy loss 0.08687\n",
      "Updates 15200, num timesteps 1216080, FPS 148, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.78578, value loss 0.00669, policy loss -0.03757\n",
      "Updates 15300, num timesteps 1224080, FPS 148, mean/median reward -19.2/-19.0, min/max reward -21.0/-17.0, entropy 1.78058, value loss 0.02990, policy loss 0.03569\n",
      "Updates 15400, num timesteps 1232080, FPS 148, mean/median reward -19.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78541, value loss 0.00881, policy loss 0.00098\n",
      "Updates 15500, num timesteps 1240080, FPS 148, mean/median reward -19.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78712, value loss 0.01946, policy loss 0.01279\n",
      "Updates 15600, num timesteps 1248080, FPS 148, mean/median reward -19.5/-20.0, min/max reward -21.0/-18.0, entropy 1.78105, value loss 0.01738, policy loss -0.06556\n",
      "Updates 15700, num timesteps 1256080, FPS 148, mean/median reward -19.6/-20.0, min/max reward -21.0/-17.0, entropy 1.78564, value loss 0.03400, policy loss 0.02907\n",
      "Updates 15800, num timesteps 1264080, FPS 148, mean/median reward -19.5/-20.0, min/max reward -21.0/-17.0, entropy 1.78502, value loss 0.00785, policy loss 0.01789\n",
      "Updates 15900, num timesteps 1272080, FPS 148, mean/median reward -19.4/-20.0, min/max reward -21.0/-18.0, entropy 1.78522, value loss 0.01716, policy loss -0.02235\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 16000, num timesteps 1280080, FPS 148, mean/median reward -19.2/-19.0, min/max reward -21.0/-18.0, entropy 1.77739, value loss 0.05534, policy loss 0.07931\n",
      "Updates 16100, num timesteps 1288080, FPS 148, mean/median reward -19.1/-19.0, min/max reward -21.0/-17.0, entropy 1.78563, value loss 0.00846, policy loss 0.01663\n",
      "Updates 16200, num timesteps 1296080, FPS 149, mean/median reward -19.0/-19.0, min/max reward -21.0/-17.0, entropy 1.78645, value loss 0.01904, policy loss -0.04064\n",
      "Updates 16300, num timesteps 1304080, FPS 149, mean/median reward -18.9/-19.0, min/max reward -21.0/-17.0, entropy 1.77025, value loss 0.01741, policy loss -0.09657\n",
      "Updates 16400, num timesteps 1312080, FPS 149, mean/median reward -19.0/-19.0, min/max reward -21.0/-17.0, entropy 1.77361, value loss 0.04516, policy loss -0.00749\n",
      "Updates 16500, num timesteps 1320080, FPS 149, mean/median reward -19.2/-20.0, min/max reward -21.0/-17.0, entropy 1.78163, value loss 0.02021, policy loss 0.05186\n",
      "Updates 16600, num timesteps 1328080, FPS 149, mean/median reward -19.4/-20.0, min/max reward -21.0/-17.0, entropy 1.76080, value loss 0.01131, policy loss 0.00646\n",
      "Updates 16700, num timesteps 1336080, FPS 149, mean/median reward -19.2/-19.0, min/max reward -21.0/-17.0, entropy 1.76707, value loss 0.03297, policy loss 0.07968\n",
      "Updates 16800, num timesteps 1344080, FPS 149, mean/median reward -18.6/-19.0, min/max reward -20.0/-15.0, entropy 1.78171, value loss 0.01653, policy loss -0.05162\n",
      "Updates 16900, num timesteps 1352080, FPS 149, mean/median reward -18.2/-19.0, min/max reward -20.0/-15.0, entropy 1.77652, value loss 0.03524, policy loss -0.03884\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 17000, num timesteps 1360080, FPS 149, mean/median reward -18.6/-19.0, min/max reward -20.0/-17.0, entropy 1.77211, value loss 0.07166, policy loss -0.04417\n",
      "Updates 17100, num timesteps 1368080, FPS 149, mean/median reward -18.8/-19.0, min/max reward -21.0/-17.0, entropy 1.77714, value loss 0.02465, policy loss -0.01290\n",
      "Updates 17200, num timesteps 1376080, FPS 149, mean/median reward -19.1/-19.0, min/max reward -21.0/-16.0, entropy 1.78893, value loss 0.00749, policy loss -0.01983\n",
      "Updates 17300, num timesteps 1384080, FPS 149, mean/median reward -18.9/-20.0, min/max reward -21.0/-15.0, entropy 1.77603, value loss 0.01488, policy loss -0.04209\n",
      "Updates 17400, num timesteps 1392080, FPS 149, mean/median reward -18.8/-20.0, min/max reward -21.0/-15.0, entropy 1.76885, value loss 0.01181, policy loss -0.04192\n",
      "Updates 17500, num timesteps 1400080, FPS 149, mean/median reward -18.9/-19.0, min/max reward -21.0/-17.0, entropy 1.78698, value loss 0.01828, policy loss -0.03473\n",
      "Updates 17600, num timesteps 1408080, FPS 149, mean/median reward -18.7/-19.0, min/max reward -21.0/-17.0, entropy 1.78009, value loss 0.02934, policy loss -0.08713\n",
      "Updates 17700, num timesteps 1416080, FPS 149, mean/median reward -18.8/-19.0, min/max reward -21.0/-16.0, entropy 1.75978, value loss 0.01297, policy loss 0.01121\n",
      "Updates 17800, num timesteps 1424080, FPS 149, mean/median reward -18.8/-19.0, min/max reward -21.0/-16.0, entropy 1.76153, value loss 0.02032, policy loss 0.03492\n",
      "Updates 17900, num timesteps 1432080, FPS 149, mean/median reward -19.3/-19.0, min/max reward -21.0/-17.0, entropy 1.78602, value loss 0.02446, policy loss -0.02744\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 18000, num timesteps 1440080, FPS 149, mean/median reward -18.6/-19.0, min/max reward -21.0/-16.0, entropy 1.78008, value loss 0.03345, policy loss -0.05494\n",
      "Updates 18100, num timesteps 1448080, FPS 149, mean/median reward -18.4/-19.0, min/max reward -21.0/-16.0, entropy 1.77209, value loss 0.03752, policy loss 0.05897\n",
      "Updates 18200, num timesteps 1456080, FPS 149, mean/median reward -17.9/-18.0, min/max reward -21.0/-16.0, entropy 1.78028, value loss 0.02088, policy loss -0.00928\n",
      "Updates 18300, num timesteps 1464080, FPS 149, mean/median reward -18.6/-19.0, min/max reward -21.0/-16.0, entropy 1.74748, value loss 0.06794, policy loss 0.03709\n",
      "Updates 18400, num timesteps 1472080, FPS 149, mean/median reward -19.0/-19.0, min/max reward -21.0/-17.0, entropy 1.77787, value loss 0.02759, policy loss -0.06580\n",
      "Updates 18500, num timesteps 1480080, FPS 150, mean/median reward -19.0/-19.0, min/max reward -21.0/-17.0, entropy 1.77791, value loss 0.00894, policy loss -0.01137\n",
      "Updates 18600, num timesteps 1488080, FPS 150, mean/median reward -18.7/-19.0, min/max reward -21.0/-16.0, entropy 1.77200, value loss 0.00874, policy loss -0.02943\n",
      "Updates 18700, num timesteps 1496080, FPS 150, mean/median reward -18.2/-19.0, min/max reward -20.0/-16.0, entropy 1.75765, value loss 0.03505, policy loss -0.00642\n",
      "Updates 18800, num timesteps 1504080, FPS 150, mean/median reward -18.1/-19.0, min/max reward -20.0/-16.0, entropy 1.77179, value loss 0.03421, policy loss 0.14197\n",
      "Updates 18900, num timesteps 1512080, FPS 150, mean/median reward -17.9/-18.0, min/max reward -19.0/-16.0, entropy 1.77930, value loss 0.03761, policy loss 0.10229\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 19000, num timesteps 1520080, FPS 150, mean/median reward -17.7/-18.0, min/max reward -20.0/-15.0, entropy 1.76927, value loss 0.02887, policy loss -0.04481\n",
      "Updates 19100, num timesteps 1528080, FPS 150, mean/median reward -18.1/-18.0, min/max reward -21.0/-15.0, entropy 1.76414, value loss 0.02581, policy loss -0.04162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-101:\n",
      "Process Process-105:\n",
      "Process Process-103:\n",
      "Process Process-99:\n",
      "Process Process-100:\n",
      "Process Process-107:\n",
      "Process Process-104:\n",
      "Process Process-109:\n",
      "Process Process-106:\n",
      "Process Process-98:\n",
      "Traceback (most recent call last):\n",
      "Process Process-111:\n",
      "Process Process-97:\n",
      "Process Process-108:\n",
      "Process Process-102:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-112:\n",
      "Process Process-110:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/beans/pytorch-a2c-ppo-acktr/all_stuff.py\", line 8, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4cc4f14015d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-4cc4f14015d0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_loss_coef\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a2c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"#######\")\n",
    "    print(\"WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\")\n",
    "    print(\"#######\")\n",
    "\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    if args.vis:\n",
    "        from visdom import Visdom\n",
    "        viz = Visdom()\n",
    "        win = None\n",
    "\n",
    "    envs = [make_env(args.env_name, args.seed, i, args.log_dir)\n",
    "                for i in range(args.num_processes)]\n",
    "\n",
    "    if args.num_processes > 1:\n",
    "        envs = SubprocVecEnv(envs)\n",
    "    else:\n",
    "        envs = DummyVecEnv(envs)\n",
    "\n",
    "    if len(envs.observation_space.shape) == 1:\n",
    "        envs = VecNormalize(envs)\n",
    "\n",
    "    obs_shape = envs.observation_space.shape\n",
    "    obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "\n",
    "    if len(envs.observation_space.shape) == 3:\n",
    "        actor_critic = CNNPolicy(obs_shape[0], envs.action_space, args.recurrent_policy)\n",
    "    else:\n",
    "        assert not args.recurrent_policy, \\\n",
    "            \"Recurrent policy is not implemented for the MLP controller\"\n",
    "        actor_critic = MLPPolicy(obs_shape[0], envs.action_space)\n",
    "\n",
    "    if envs.action_space.__class__.__name__ == \"Discrete\":\n",
    "        action_shape = 1\n",
    "    else:\n",
    "        action_shape = envs.action_space.shape[0]\n",
    "\n",
    "    if args.cuda:\n",
    "        actor_critic.cuda()\n",
    "\n",
    "    if args.algo == 'a2c':\n",
    "        optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "    elif args.algo == 'ppo':\n",
    "        optimizer = optim.Adam(actor_critic.parameters(), args.lr, eps=args.eps)\n",
    "    elif args.algo == 'acktr':\n",
    "        optimizer = KFACOptimizer(actor_critic)\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space,\\\n",
    "                              actor_critic.state_size)\n",
    "    current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "    def update_current_obs(obs):\n",
    "        shape_dim0 = envs.observation_space.shape[0]\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        if args.num_stack > 1:\n",
    "            current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "    obs = envs.reset()\n",
    "    update_current_obs(obs)\n",
    "\n",
    "    rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "    # These variables are used to compute average rewards for all processes.\n",
    "    episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "    final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "    if args.cuda:\n",
    "        current_obs = current_obs.cuda()\n",
    "        rollouts.cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(args.num_steps):\n",
    "            # Sample actions\n",
    "            value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                      Variable(rollouts.states[step], volatile=True),\n",
    "                                                                      Variable(rollouts.masks[step], volatile=True))\n",
    "            cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "            episode_rewards += reward\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            final_rewards *= masks\n",
    "            final_rewards += (1 - masks) * episode_rewards\n",
    "            episode_rewards *= masks\n",
    "\n",
    "            if args.cuda:\n",
    "                masks = masks.cuda()\n",
    "\n",
    "            if current_obs.dim() == 4:\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "            else:\n",
    "                current_obs *= masks\n",
    "\n",
    "            update_current_obs(obs)\n",
    "            rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks)\n",
    "\n",
    "        next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                                  Variable(rollouts.states[-1], volatile=True),\n",
    "                                  Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "        if args.algo in ['a2c', 'acktr']:\n",
    "            values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                           Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                           Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                           Variable(rollouts.actions.view(-1, action_shape)))\n",
    "\n",
    "            values = values.view(args.num_steps, args.num_processes, 1)\n",
    "            action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "\n",
    "            advantages = Variable(rollouts.returns[:-1]) - values\n",
    "            value_loss = advantages.pow(2).mean()\n",
    "\n",
    "            action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "            if args.algo == 'acktr' and optimizer.steps % optimizer.Ts == 0:\n",
    "                # Sampled fisher, see Martens 2014\n",
    "                actor_critic.zero_grad()\n",
    "                pg_fisher_loss = -action_log_probs.mean()\n",
    "\n",
    "                value_noise = Variable(torch.randn(values.size()))\n",
    "                if args.cuda:\n",
    "                    value_noise = value_noise.cuda()\n",
    "\n",
    "                sample_values = values + value_noise\n",
    "                vf_fisher_loss = -(values - Variable(sample_values.data)).pow(2).mean()\n",
    "\n",
    "                fisher_loss = pg_fisher_loss + vf_fisher_loss\n",
    "                optimizer.acc_stats = True\n",
    "                fisher_loss.backward(retain_graph=True)\n",
    "                optimizer.acc_stats = False\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            (value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef).backward()\n",
    "\n",
    "            if args.algo == 'a2c':\n",
    "                nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % args.save_interval == 0 and args.save_dir != \"\":\n",
    "            \n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            # A really ugly way to save a model to CPU\n",
    "            save_model = actor_critic\n",
    "            if args.cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "                \n",
    "\n",
    "        if j % args.log_interval == 0:\n",
    "            end = time.time()\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "                format(j, total_num_steps,\n",
    "                       int(total_num_steps / (end - start)),\n",
    "                       final_rewards.mean(),\n",
    "                       final_rewards.median(),\n",
    "                       final_rewards.min(),\n",
    "                       final_rewards.max(), dist_entropy.data[0],\n",
    "                       value_loss.data[0], action_loss.data[0]))\n",
    "        if args.vis and j % args.vis_interval == 0:\n",
    "            try:\n",
    "                # Sometimes monitor doesn't properly flush the outputs\n",
    "                win = visdom_plot(viz, win, args.log_dir, args.env_name, args.algo)\n",
    "            except IOError:\n",
    "                pass\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actor_critic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c487f548e1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'actor_critic' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
