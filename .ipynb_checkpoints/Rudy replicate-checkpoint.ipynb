{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from arguments import get_args\n",
    "\n",
    "#from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "#from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "#from baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "from all_stuff import * # this has the above modules consolidated into a single file. god this was a bitch\n",
    "\n",
    "from envs import make_env # had to manually add some files into directory for env to reference bc baselines \n",
    "# modules not working right\n",
    "\n",
    "from kfac import KFACOptimizer\n",
    "from model import CNNPolicy, MLPPolicy\n",
    "from storage import RolloutStorage\n",
    "from visualize import visdom_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=False\n",
    "        self.algo='a2c'\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=16\n",
    "        self.recurrent_policy=False\n",
    "        self.vis=False\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=1e6\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.vis_interval=100\n",
    "        self.from_saved_model=True\n",
    "        \n",
    "args = args()\n",
    "\n",
    "save_path = os.path.join(args.save_dir, args.algo)\n",
    "SAVE_PATH = os.path.join(save_path, args.env_name + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######\n",
      "WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\n",
      "#######\n",
      "loading saved model from  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "\n",
      "############# NEW STEP ######################\n",
      "\n",
      "\n",
      "these come from initial actor_critic.act\n",
      "cpu_actions [4 3] \n",
      "value Variable containing:\n",
      "-0.1040\n",
      "-0.1040\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "states Variable containing:\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "\n",
      "\n",
      "these come after we've taken a step and observed reward and next obs\n",
      " obs shape (2, 1, 84, 84) \n",
      "reward \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "episode_rewards \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "done [False False]\n",
      "\n",
      "############# NEW STEP ######################\n",
      "\n",
      "\n",
      "these come from initial actor_critic.act\n",
      "cpu_actions [0 0] \n",
      "value Variable containing:\n",
      "-0.2085\n",
      "-0.2397\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "states Variable containing:\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "\n",
      "\n",
      "these come after we've taken a step and observed reward and next obs\n",
      " obs shape (2, 1, 84, 84) \n",
      "reward \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "episode_rewards \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "done [False False]\n",
      "\n",
      "############# NEW STEP ######################\n",
      "\n",
      "\n",
      "these come from initial actor_critic.act\n",
      "cpu_actions [1 4] \n",
      "value Variable containing:\n",
      "-0.2397\n",
      "-0.2397\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "states Variable containing:\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "\n",
      "\n",
      "these come after we've taken a step and observed reward and next obs\n",
      " obs shape (2, 1, 84, 84) \n",
      "reward \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "episode_rewards \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "done [False False]\n",
      "\n",
      "############# NEW STEP ######################\n",
      "\n",
      "\n",
      "these come from initial actor_critic.act\n",
      "cpu_actions [1 3] \n",
      "value Variable containing:\n",
      "-0.2397\n",
      "-0.2397\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "states Variable containing:\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "\n",
      "\n",
      "these come after we've taken a step and observed reward and next obs\n",
      " obs shape (2, 1, 84, 84) \n",
      "reward \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "episode_rewards \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      " \n",
      "done [False False]\n",
      "\n",
      "############# DONE W STEPS ######################\n",
      "\n",
      "\n",
      "next_value, taken from actor_critic \n",
      "-0.2397\n",
      "-0.2397\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "\n",
      "Rollout action log probs, dist entropy and values\n",
      " Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.9267\n",
      " -1.8154\n",
      "\n",
      "(1 ,.,.) = \n",
      " -1.7552\n",
      " -1.7268\n",
      "\n",
      "(2 ,.,.) = \n",
      " -1.7755\n",
      " -1.7752\n",
      "\n",
      "(3 ,.,.) = \n",
      " -1.7755\n",
      " -1.7834\n",
      "[torch.FloatTensor of size 4x2x1]\n",
      " Variable containing:\n",
      " 1.7880\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1040\n",
      " -0.1040\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.2085\n",
      " -0.2397\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.2397\n",
      " -0.2397\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.2397\n",
      " -0.2397\n",
      "[torch.FloatTensor of size 4x2x1]\n",
      "\n",
      "\n",
      " rollout returns, after compute. These are discounted backwards from next_value \n",
      "(0 ,.,.) = \n",
      " -0.2303\n",
      " -0.2303\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.2326\n",
      " -0.2326\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.2349\n",
      " -0.2349\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.2373\n",
      " -0.2373\n",
      "[torch.FloatTensor of size 4x2x1]\n",
      "\n",
      "\n",
      "advantages. Equals rollout_returns - values Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1263\n",
      " -0.1263\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0241\n",
      "  0.0071\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.0048\n",
      "  0.0048\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.0024\n",
      "  0.0024\n",
      "[torch.FloatTensor of size 4x2x1]\n",
      "\n",
      "\n",
      "value_loss (mean of squared Advantages), action_loss (Advantages * action_log_probs) Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.0721\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      "1.00000e-02 *\n",
      " -5.9624\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "there are no graph nodes that require computing gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1c756d439d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-1c756d439d2a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# summing up total loss, then going backwards on it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_loss_coef\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: there are no graph nodes that require computing gradients"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"#######\")\n",
    "    print(\"WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\")\n",
    "    print(\"#######\")\n",
    "\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    if args.vis:\n",
    "        from visdom import Visdom\n",
    "        viz = Visdom()\n",
    "        win = None\n",
    "\n",
    "    envs = [make_env(args.env_name, args.seed, i, args.log_dir)\n",
    "                for i in range(args.num_processes)]\n",
    "\n",
    "    if args.num_processes > 1:\n",
    "        envs = SubprocVecEnv(envs)\n",
    "    else:\n",
    "        envs = DummyVecEnv(envs)\n",
    "\n",
    "    if len(envs.observation_space.shape) == 1:\n",
    "        envs = VecNormalize(envs)\n",
    "\n",
    "    obs_shape = envs.observation_space.shape\n",
    "    obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "\n",
    "    if args.from_saved_model:\n",
    "        print(\"loading saved model from \", SAVE_PATH)\n",
    "        actor_critic = torch.load(SAVE_PATH)\n",
    "    else:\n",
    "        if len(envs.observation_space.shape) == 3:\n",
    "            actor_critic = CNNPolicy(obs_shape[0], envs.action_space, args.recurrent_policy)\n",
    "        else:\n",
    "            assert not args.recurrent_policy, \\\n",
    "                \"Recurrent policy is not implemented for the MLP controller\"\n",
    "            actor_critic = MLPPolicy(obs_shape[0], envs.action_space)\n",
    "\n",
    "    if envs.action_space.__class__.__name__ == \"Discrete\":\n",
    "        action_shape = 1\n",
    "    else:\n",
    "        action_shape = envs.action_space.shape[0]\n",
    "\n",
    "    if args.cuda:\n",
    "        actor_critic.cuda()\n",
    "\n",
    "    optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "\n",
    "    global rollouts\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space,\\\n",
    "                              actor_critic.state_size)\n",
    "    current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "    def update_current_obs(obs):\n",
    "        shape_dim0 = envs.observation_space.shape[0]\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        if args.num_stack > 1:\n",
    "            current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "    obs = envs.reset()\n",
    "    update_current_obs(obs)\n",
    "\n",
    "    rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "    # These variables are used to compute average rewards for all processes.\n",
    "    episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "    final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "    if args.cuda:\n",
    "        current_obs = current_obs.cuda()\n",
    "        rollouts.cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(args.num_steps):\n",
    "            \n",
    "            #print(\"\\n############# NEW STEP ######################\\n\")\n",
    "            \n",
    "            # Sample actions\n",
    "            # look at the state, predict value and actions\n",
    "            # Using model in predict mode. Sampling actions from the distribution, also getting a value\n",
    "            # which we're not using (unless gae)\n",
    "            value, action, action_log_prob, dist_entropy, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                      Variable(rollouts.states[step], volatile=True),\n",
    "                                                                      Variable(rollouts.masks[step], volatile=True))\n",
    "        \n",
    "            cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "            \n",
    "            #print(\"\\nthese come from initial actor_critic.act\\ncpu_actions\", cpu_actions, \"\\nvalue\", value, \"\\nstates\", states)\n",
    "\n",
    "            # Take the actions in the environments, Obser reward and next obs\n",
    "            # obs is single frame for each actor\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "            episode_rewards += reward\n",
    "            \n",
    "            #print(\"\\n\\nthese come after we've taken a step and observed reward and next obs\\n\",\\\n",
    "            #     \"obs shape\", obs.shape, \"\\nreward\", reward,\"\\nepisode_rewards\", episode_rewards, \"\\ndone\", done)\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            # if final, zero out the rewards then fill them in with episode_rewards, then use masks to\n",
    "            # reset episode_rewards to zero for the actors that just finished.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            final_rewards *= masks\n",
    "            final_rewards += (1 - masks) * episode_rewards\n",
    "            episode_rewards *= masks\n",
    "\n",
    "            if args.cuda:\n",
    "                masks = masks.cuda()\n",
    "\n",
    "            if current_obs.dim() == 4:\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "            else:\n",
    "                current_obs *= masks\n",
    "\n",
    "            update_current_obs(obs)\n",
    "            # this returns four frames for each actor, \n",
    "            #print(\"\\ncurrent_obs\", current_obs)\n",
    "            \n",
    "            # the value we're entering is value pred. Do we use this for calculating Advantage? \n",
    "            # i don't believe so. I believe it's only relevent when using gae\n",
    "            rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks, dist_entropy.data)\n",
    "\n",
    "        \n",
    "        #print(\"\\n############# DONE W STEPS ######################\\n\")   \n",
    "        # estimating the value of the last state, V(S). We'll discount this backwards \n",
    "        # and add it to discounted rewards to get the total returns for each step\n",
    "        \n",
    "        next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                                  Variable(rollouts.states[-1], volatile=True),\n",
    "                                  Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "        #print(\"\\nnext_value, taken from actor_critic\", next_value)\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "        \n",
    "        \n",
    "        # this predicts values for each state. Why don't we just gather values as we're\n",
    "        # collecting SAR touples? Because we need to be able to go backwards on them.\n",
    "        values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                       Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                       Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                       Variable(rollouts.actions.view(-1, action_shape)))\n",
    "        \n",
    "        values = values.view(args.num_steps, args.num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "        \n",
    "        #print(\"\\naction_log_probs\", action_log_probs, \"\\ndist_entropy\", dist_entropy)\n",
    "        \n",
    "        #print(\"\\nvalues\", values)\n",
    "        \n",
    "        # these are the values we gathered during acting. They appear identical to those generated\n",
    "        # above. \n",
    "\n",
    "        \"\"\"\n",
    "        action_log_probs = Variable(rollouts.action_log_probs) # this is 4X2X1 \n",
    "        dist_entropy = Variable(torch.FloatTensor([rollouts.dist_entropy.mean()])) # should be a single number\n",
    "        values = Variable(rollouts.value_preds[:-1]) # chop off end \n",
    "        print(\"\\nRollout action log probs, dist entropy and values\\n\", action_log_probs, dist_entropy, values)\"\"\"\n",
    "        \n",
    "        rollout_returns = rollouts.returns[:-1]\n",
    "        advantages = Variable(rollout_returns) - values\n",
    "        \n",
    "        #print(\"\\n rollout returns, after compute. These are discounted backwards from next_value\", rollout_returns)\n",
    "        \n",
    "        #print(\"\\nadvantages. Equals rollout_returns - values\", advantages)\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "        \n",
    "        #print(\"\\nvalue_loss (mean of squared Advantages), action_loss (Advantages * action_log_probs)\", value_loss, action_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # summing up total loss, then going backwards on it.\n",
    "        (value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef).backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "        #print(\"stepping the optimizer with value loss, action loss, and entropy\")\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % args.save_interval == 0 and args.save_dir != \"\":\n",
    "            \n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            # A really ugly way to save a model to CPU\n",
    "            save_model = actor_critic\n",
    "            if args.cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "                \n",
    "            torch.save(save_model, SAVE_PATH)\n",
    "            print(\"model saved to \", SAVE_PATH)\n",
    "\n",
    "        if j % args.log_interval == 0:\n",
    "            end = time.time()\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "                format(j, total_num_steps,\n",
    "                       int(total_num_steps / (end - start)),\n",
    "                       final_rewards.mean(),\n",
    "                       final_rewards.median(),\n",
    "                       final_rewards.min(),\n",
    "                       final_rewards.max(), dist_entropy.data[0],\n",
    "                       value_loss.data[0], action_loss.data[0]))\n",
    "        if args.vis and j % args.vis_interval == 0:\n",
    "            try:\n",
    "                # Sometimes monitor doesn't properly flush the outputs\n",
    "                win = visdom_plot(viz, win, args.log_dir, args.env_name, args.algo)\n",
    "            except IOError:\n",
    "                pass\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.7887 -1.8077\n",
       "-1.7896 -1.7960\n",
       "-1.8077 -1.7887\n",
       "-1.7960 -1.7960\n",
       "[torch.FloatTensor of size 4x2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_log_probs_.view(args.num_steps, args.num_processes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
