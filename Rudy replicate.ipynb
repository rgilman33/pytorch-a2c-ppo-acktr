{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py:680: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  not cbook.is_string_like(colors[0]):\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from arguments import get_args\n",
    "\n",
    "#from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "#from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "#from baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "from all_stuff import * # this has the above modules consolidated into a single file. god this was a bitch\n",
    "\n",
    "from envs import make_env # had to manually add some files into directory for env to reference bc baselines \n",
    "# modules not working right\n",
    "\n",
    "from kfac import KFACOptimizer\n",
    "from model import CNNPolicy, MLPPolicy\n",
    "from storage import RolloutStorage\n",
    "from visualize import visdom_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=True\n",
    "        self.algo='a2c'\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=16\n",
    "        self.recurrent_policy=False\n",
    "        self.vis=False\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=3e6\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.vis_interval=100\n",
    "        self.from_saved_model=True\n",
    "        \n",
    "args = args()\n",
    "\n",
    "save_path = os.path.join(args.save_dir, args.algo)\n",
    "SAVE_PATH = os.path.join(save_path, args.env_name + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######\n",
      "WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\n",
      "#######\n",
      "loading saved model from  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 0, num timesteps 80, FPS 623, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.78588, value loss 0.00337, policy loss -0.02857\n",
      "Updates 100, num timesteps 8080, FPS 968, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.76185, value loss 0.03304, policy loss 0.06543\n",
      "Updates 200, num timesteps 16080, FPS 943, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.66368, value loss 0.01249, policy loss -0.03710\n",
      "Updates 300, num timesteps 24080, FPS 939, mean/median reward -1.0/0.0, min/max reward -16.0/0.0, entropy 1.76785, value loss 0.02095, policy loss 0.05166\n",
      "Updates 400, num timesteps 32080, FPS 933, mean/median reward -1.0/0.0, min/max reward -16.0/0.0, entropy 1.76692, value loss 0.00879, policy loss 0.02968\n",
      "Updates 500, num timesteps 40080, FPS 930, mean/median reward -5.0/0.0, min/max reward -16.0/0.0, entropy 1.71886, value loss 0.02025, policy loss 0.01867\n",
      "Updates 600, num timesteps 48080, FPS 929, mean/median reward -9.4/-9.0, min/max reward -16.0/0.0, entropy 1.70537, value loss 0.04080, policy loss -0.11852\n",
      "Updates 700, num timesteps 56080, FPS 930, mean/median reward -9.2/-9.0, min/max reward -16.0/2.0, entropy 1.77304, value loss 0.01510, policy loss -0.00788\n",
      "Updates 800, num timesteps 64080, FPS 930, mean/median reward -9.0/-9.0, min/max reward -13.0/2.0, entropy 1.77782, value loss 0.01083, policy loss -0.01634\n",
      "Updates 900, num timesteps 72080, FPS 931, mean/median reward -9.0/-9.0, min/max reward -13.0/2.0, entropy 1.75310, value loss 0.01455, policy loss -0.06795\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 1000, num timesteps 80080, FPS 930, mean/median reward -8.8/-9.0, min/max reward -13.0/2.0, entropy 1.68460, value loss 0.02985, policy loss -0.12969\n",
      "Updates 1100, num timesteps 88080, FPS 929, mean/median reward -7.8/-9.0, min/max reward -12.0/2.0, entropy 1.72717, value loss 0.01695, policy loss 0.01080\n",
      "Updates 1200, num timesteps 96080, FPS 929, mean/median reward -6.6/-7.0, min/max reward -12.0/2.0, entropy 1.72396, value loss 0.01042, policy loss 0.00349\n",
      "Updates 1300, num timesteps 104080, FPS 929, mean/median reward -6.6/-7.0, min/max reward -12.0/2.0, entropy 1.78643, value loss 0.00717, policy loss -0.00219\n",
      "Updates 1400, num timesteps 112080, FPS 929, mean/median reward -6.6/-7.0, min/max reward -10.0/-2.0, entropy 1.69111, value loss 0.03289, policy loss 0.06034\n",
      "Updates 1500, num timesteps 120080, FPS 929, mean/median reward -6.6/-7.0, min/max reward -10.0/-2.0, entropy 1.75710, value loss 0.02183, policy loss -0.03939\n",
      "Updates 1600, num timesteps 128080, FPS 928, mean/median reward -6.4/-7.0, min/max reward -10.0/-2.0, entropy 1.68700, value loss 0.01252, policy loss 0.00749\n",
      "Updates 1700, num timesteps 136080, FPS 928, mean/median reward -6.2/-6.0, min/max reward -10.0/-2.0, entropy 1.72954, value loss 0.06203, policy loss 0.02153\n",
      "Updates 1800, num timesteps 144080, FPS 928, mean/median reward -5.2/-6.0, min/max reward -10.0/-2.0, entropy 1.74407, value loss 0.01622, policy loss -0.03261\n",
      "Updates 1900, num timesteps 152080, FPS 928, mean/median reward -5.1/-6.0, min/max reward -10.0/3.0, entropy 1.77227, value loss 0.03454, policy loss 0.04091\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 2000, num timesteps 160080, FPS 928, mean/median reward -5.1/-6.0, min/max reward -10.0/3.0, entropy 1.69017, value loss 0.00842, policy loss 0.05533\n",
      "Updates 2100, num timesteps 168080, FPS 927, mean/median reward -5.1/-6.0, min/max reward -10.0/3.0, entropy 1.68673, value loss 0.10950, policy loss -0.00877\n",
      "Updates 2200, num timesteps 176080, FPS 927, mean/median reward -5.2/-6.0, min/max reward -10.0/3.0, entropy 1.74909, value loss 0.02535, policy loss 0.04907\n",
      "Updates 2300, num timesteps 184080, FPS 925, mean/median reward -5.1/-4.0, min/max reward -13.0/3.0, entropy 1.78223, value loss 0.01620, policy loss -0.04884\n",
      "Updates 2400, num timesteps 192080, FPS 925, mean/median reward -5.9/-6.0, min/max reward -13.0/3.0, entropy 1.76440, value loss 0.01058, policy loss -0.04525\n",
      "Updates 2500, num timesteps 200080, FPS 925, mean/median reward -5.9/-6.0, min/max reward -13.0/3.0, entropy 1.74093, value loss 0.02757, policy loss 0.06418\n",
      "Updates 2600, num timesteps 208080, FPS 925, mean/median reward -6.6/-6.0, min/max reward -13.0/-2.0, entropy 1.77437, value loss 0.01966, policy loss -0.05466\n",
      "Updates 2700, num timesteps 216080, FPS 925, mean/median reward -7.6/-8.0, min/max reward -13.0/-2.0, entropy 1.77952, value loss 0.01339, policy loss 0.04035\n",
      "Updates 2800, num timesteps 224080, FPS 925, mean/median reward -8.9/-10.0, min/max reward -14.0/-1.0, entropy 1.66018, value loss 0.02180, policy loss -0.01166\n",
      "Updates 2900, num timesteps 232080, FPS 925, mean/median reward -8.4/-10.0, min/max reward -14.0/1.0, entropy 1.71059, value loss 0.00917, policy loss 0.04033\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 3000, num timesteps 240080, FPS 924, mean/median reward -8.1/-10.0, min/max reward -14.0/1.0, entropy 1.71191, value loss 0.05008, policy loss -0.09024\n",
      "Updates 3100, num timesteps 248080, FPS 925, mean/median reward -7.9/-10.0, min/max reward -14.0/1.0, entropy 1.75307, value loss 0.01475, policy loss 0.03976\n",
      "Updates 3200, num timesteps 256080, FPS 924, mean/median reward -7.6/-10.0, min/max reward -14.0/1.0, entropy 1.75613, value loss 0.00570, policy loss 0.02688\n",
      "Updates 3300, num timesteps 264080, FPS 924, mean/median reward -6.9/-8.0, min/max reward -14.0/1.0, entropy 1.76944, value loss 0.01440, policy loss 0.06896\n",
      "Updates 3400, num timesteps 272080, FPS 924, mean/median reward -6.4/-8.0, min/max reward -12.0/1.0, entropy 1.70413, value loss 0.07425, policy loss -0.09393\n",
      "Updates 3500, num timesteps 280080, FPS 924, mean/median reward -6.4/-8.0, min/max reward -12.0/1.0, entropy 1.77638, value loss 0.01075, policy loss -0.01234\n",
      "Updates 3600, num timesteps 288080, FPS 924, mean/median reward -6.6/-8.0, min/max reward -12.0/2.0, entropy 1.70558, value loss 0.02175, policy loss -0.02368\n",
      "Updates 3700, num timesteps 296080, FPS 924, mean/median reward -6.6/-8.0, min/max reward -12.0/2.0, entropy 1.74949, value loss 0.01381, policy loss 0.06582\n",
      "Updates 3800, num timesteps 304080, FPS 924, mean/median reward -5.9/-7.0, min/max reward -12.0/2.0, entropy 1.77186, value loss 0.01165, policy loss -0.00590\n",
      "Updates 3900, num timesteps 312080, FPS 924, mean/median reward -5.2/-5.0, min/max reward -12.0/2.0, entropy 1.75177, value loss 0.02531, policy loss -0.07499\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 4000, num timesteps 320080, FPS 924, mean/median reward -4.8/-5.0, min/max reward -12.0/2.0, entropy 1.74771, value loss 0.02372, policy loss 0.00308\n",
      "Updates 4100, num timesteps 328080, FPS 924, mean/median reward -4.8/-5.0, min/max reward -12.0/2.0, entropy 1.75195, value loss 0.01960, policy loss 0.03766\n",
      "Updates 4200, num timesteps 336080, FPS 924, mean/median reward -4.6/-5.0, min/max reward -11.0/-1.0, entropy 1.72253, value loss 0.02446, policy loss -0.04033\n",
      "Updates 4300, num timesteps 344080, FPS 924, mean/median reward -4.8/-5.0, min/max reward -11.0/-1.0, entropy 1.76155, value loss 0.01767, policy loss -0.02959\n",
      "Updates 4400, num timesteps 352080, FPS 924, mean/median reward -5.0/-5.0, min/max reward -11.0/-1.0, entropy 1.72546, value loss 0.06378, policy loss -0.05669\n",
      "Updates 4500, num timesteps 360080, FPS 924, mean/median reward -5.2/-5.0, min/max reward -10.0/-1.0, entropy 1.77165, value loss 0.01302, policy loss -0.03102\n",
      "Updates 4600, num timesteps 368080, FPS 925, mean/median reward -4.9/-5.0, min/max reward -10.0/6.0, entropy 1.76197, value loss 0.01312, policy loss -0.05442\n",
      "Updates 4700, num timesteps 376080, FPS 925, mean/median reward -4.3/-5.0, min/max reward -10.0/6.0, entropy 1.73201, value loss 0.02193, policy loss -0.05392\n",
      "Updates 4800, num timesteps 384080, FPS 925, mean/median reward -4.6/-6.0, min/max reward -10.0/6.0, entropy 1.75186, value loss 0.02103, policy loss -0.00945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 4900, num timesteps 392080, FPS 925, mean/median reward -4.3/-5.0, min/max reward -10.0/6.0, entropy 1.76280, value loss 0.00785, policy loss 0.01631\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 5000, num timesteps 400080, FPS 925, mean/median reward -4.2/-6.0, min/max reward -10.0/6.0, entropy 1.78170, value loss 0.03444, policy loss -0.03453\n",
      "Updates 5100, num timesteps 408080, FPS 925, mean/median reward -3.8/-5.0, min/max reward -9.0/3.0, entropy 1.77313, value loss 0.05473, policy loss -0.01236\n",
      "Updates 5200, num timesteps 416080, FPS 925, mean/median reward -5.2/-6.0, min/max reward -12.0/3.0, entropy 1.74454, value loss 0.09082, policy loss -0.12794\n",
      "Updates 5300, num timesteps 424080, FPS 925, mean/median reward -6.0/-7.0, min/max reward -12.0/3.0, entropy 1.72363, value loss 0.01109, policy loss -0.05529\n",
      "Updates 5400, num timesteps 432080, FPS 925, mean/median reward -6.2/-7.0, min/max reward -12.0/3.0, entropy 1.77720, value loss 0.02780, policy loss 0.06339\n",
      "Updates 5500, num timesteps 440080, FPS 925, mean/median reward -7.2/-8.0, min/max reward -12.0/3.0, entropy 1.78548, value loss 0.00487, policy loss 0.00604\n",
      "Updates 5600, num timesteps 448080, FPS 925, mean/median reward -7.9/-9.0, min/max reward -12.0/1.0, entropy 1.71717, value loss 0.05985, policy loss -0.14808\n",
      "Updates 5700, num timesteps 456080, FPS 925, mean/median reward -7.9/-8.0, min/max reward -12.0/-2.0, entropy 1.70776, value loss 0.03681, policy loss 0.04455\n",
      "Updates 5800, num timesteps 464080, FPS 925, mean/median reward -7.8/-8.0, min/max reward -12.0/-2.0, entropy 1.75759, value loss 0.06841, policy loss 0.06581\n",
      "Updates 5900, num timesteps 472080, FPS 925, mean/median reward -7.8/-8.0, min/max reward -12.0/-2.0, entropy 1.73569, value loss 0.01012, policy loss 0.02360\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 6000, num timesteps 480080, FPS 925, mean/median reward -6.5/-7.0, min/max reward -12.0/3.0, entropy 1.75996, value loss 0.03952, policy loss -0.08623\n",
      "Updates 6100, num timesteps 488080, FPS 925, mean/median reward -6.0/-7.0, min/max reward -11.0/3.0, entropy 1.78551, value loss 0.02203, policy loss 0.07323\n",
      "Updates 6200, num timesteps 496080, FPS 925, mean/median reward -6.4/-7.0, min/max reward -11.0/3.0, entropy 1.77955, value loss 0.02082, policy loss -0.02654\n",
      "Updates 6300, num timesteps 504080, FPS 925, mean/median reward -5.6/-7.0, min/max reward -10.0/3.0, entropy 1.76719, value loss 0.03522, policy loss 0.00803\n",
      "Updates 6400, num timesteps 512080, FPS 924, mean/median reward -4.8/-5.0, min/max reward -10.0/3.0, entropy 1.78400, value loss 0.06928, policy loss -0.12069\n",
      "Updates 6500, num timesteps 520080, FPS 924, mean/median reward -5.5/-7.0, min/max reward -12.0/2.0, entropy 1.77434, value loss 0.01183, policy loss 0.02611\n",
      "Updates 6600, num timesteps 528080, FPS 925, mean/median reward -5.9/-7.0, min/max reward -12.0/2.0, entropy 1.77489, value loss 0.00534, policy loss -0.05659\n",
      "Updates 6700, num timesteps 536080, FPS 925, mean/median reward -5.9/-7.0, min/max reward -12.0/2.0, entropy 1.76788, value loss 0.01110, policy loss 0.01325\n",
      "Updates 6800, num timesteps 544080, FPS 925, mean/median reward -6.2/-7.0, min/max reward -12.0/2.0, entropy 1.64200, value loss 0.02866, policy loss 0.01246\n",
      "Updates 6900, num timesteps 552080, FPS 925, mean/median reward -6.4/-7.0, min/max reward -12.0/2.0, entropy 1.70669, value loss 0.05681, policy loss 0.00557\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 7000, num timesteps 560080, FPS 925, mean/median reward -7.4/-8.0, min/max reward -12.0/1.0, entropy 1.73853, value loss 0.03102, policy loss 0.06118\n",
      "Updates 7100, num timesteps 568080, FPS 925, mean/median reward -7.7/-8.0, min/max reward -12.0/-4.0, entropy 1.68657, value loss 0.01309, policy loss 0.00435\n",
      "Updates 7200, num timesteps 576080, FPS 925, mean/median reward -7.8/-8.0, min/max reward -12.0/-4.0, entropy 1.72528, value loss 0.01721, policy loss 0.00246\n",
      "Updates 7300, num timesteps 584080, FPS 925, mean/median reward -6.8/-7.0, min/max reward -12.0/1.0, entropy 1.75282, value loss 0.01522, policy loss -0.00589\n",
      "Updates 7400, num timesteps 592080, FPS 925, mean/median reward -7.0/-7.0, min/max reward -12.0/1.0, entropy 1.78830, value loss 0.01168, policy loss -0.02061\n",
      "Updates 7500, num timesteps 600080, FPS 925, mean/median reward -7.3/-8.0, min/max reward -11.0/1.0, entropy 1.71643, value loss 0.02035, policy loss 0.00370\n",
      "Updates 7600, num timesteps 608080, FPS 925, mean/median reward -6.7/-7.0, min/max reward -11.0/1.0, entropy 1.65542, value loss 0.04194, policy loss 0.08242\n",
      "Updates 7700, num timesteps 616080, FPS 925, mean/median reward -5.4/-6.0, min/max reward -11.0/1.0, entropy 1.76889, value loss 0.01674, policy loss 0.01790\n",
      "Updates 7800, num timesteps 624080, FPS 925, mean/median reward -5.2/-6.0, min/max reward -11.0/1.0, entropy 1.75473, value loss 0.04136, policy loss -0.00455\n",
      "Updates 7900, num timesteps 632080, FPS 925, mean/median reward -5.2/-6.0, min/max reward -11.0/1.0, entropy 1.78724, value loss 0.01529, policy loss 0.02752\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 8000, num timesteps 640080, FPS 925, mean/median reward -4.9/-5.0, min/max reward -11.0/1.0, entropy 1.78673, value loss 0.00379, policy loss -0.01398\n",
      "Updates 8100, num timesteps 648080, FPS 925, mean/median reward -4.4/-5.0, min/max reward -10.0/1.0, entropy 1.75212, value loss 0.02071, policy loss 0.00451\n",
      "Updates 8200, num timesteps 656080, FPS 925, mean/median reward -5.2/-6.0, min/max reward -12.0/1.0, entropy 1.74852, value loss 0.01615, policy loss 0.01074\n",
      "Updates 8300, num timesteps 664080, FPS 925, mean/median reward -6.4/-6.0, min/max reward -12.0/-2.0, entropy 1.74870, value loss 0.01280, policy loss -0.05267\n",
      "Updates 8400, num timesteps 672080, FPS 925, mean/median reward -6.2/-7.0, min/max reward -12.0/-2.0, entropy 1.73782, value loss 0.03713, policy loss 0.10091\n",
      "Updates 8500, num timesteps 680080, FPS 925, mean/median reward -6.2/-7.0, min/max reward -12.0/-2.0, entropy 1.76658, value loss 0.03453, policy loss -0.10603\n",
      "Updates 8600, num timesteps 688080, FPS 925, mean/median reward -6.6/-8.0, min/max reward -12.0/-2.0, entropy 1.74455, value loss 0.14421, policy loss 0.00787\n",
      "Updates 8700, num timesteps 696080, FPS 925, mean/median reward -6.4/-7.0, min/max reward -12.0/-2.0, entropy 1.70564, value loss 0.01816, policy loss 0.01632\n",
      "Updates 8800, num timesteps 704080, FPS 925, mean/median reward -5.9/-7.0, min/max reward -12.0/3.0, entropy 1.78716, value loss 0.01753, policy loss -0.01269\n",
      "Updates 8900, num timesteps 712080, FPS 925, mean/median reward -5.6/-7.0, min/max reward -12.0/3.0, entropy 1.77010, value loss 0.01915, policy loss 0.02626\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 9000, num timesteps 720080, FPS 925, mean/median reward -5.6/-7.0, min/max reward -12.0/3.0, entropy 1.74736, value loss 0.10643, policy loss -0.03065\n",
      "Updates 9100, num timesteps 728080, FPS 925, mean/median reward -5.5/-6.0, min/max reward -12.0/3.0, entropy 1.77595, value loss 0.03782, policy loss 0.04603\n",
      "Updates 9200, num timesteps 736080, FPS 925, mean/median reward -5.5/-6.0, min/max reward -12.0/3.0, entropy 1.72839, value loss 0.03293, policy loss 0.01139\n",
      "Updates 9300, num timesteps 744080, FPS 925, mean/median reward -5.8/-7.0, min/max reward -12.0/3.0, entropy 1.75155, value loss 0.01628, policy loss 0.00850\n",
      "Updates 9400, num timesteps 752080, FPS 925, mean/median reward -6.8/-7.0, min/max reward -10.0/-3.0, entropy 1.76235, value loss 0.02555, policy loss 0.00418\n",
      "Updates 9500, num timesteps 760080, FPS 925, mean/median reward -6.7/-7.0, min/max reward -10.0/-3.0, entropy 1.72867, value loss 0.03592, policy loss 0.05338\n",
      "Updates 9600, num timesteps 768080, FPS 926, mean/median reward -6.7/-7.0, min/max reward -10.0/-3.0, entropy 1.76351, value loss 0.02835, policy loss -0.00542\n",
      "Updates 9700, num timesteps 776080, FPS 926, mean/median reward -6.6/-7.0, min/max reward -10.0/-3.0, entropy 1.75459, value loss 0.02465, policy loss 0.03488\n",
      "Updates 9800, num timesteps 784080, FPS 926, mean/median reward -6.2/-7.0, min/max reward -10.0/-1.0, entropy 1.74971, value loss 0.01318, policy loss -0.00925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9900, num timesteps 792080, FPS 926, mean/median reward -4.9/-6.0, min/max reward -10.0/8.0, entropy 1.75985, value loss 0.07371, policy loss 0.13967\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 10000, num timesteps 800080, FPS 926, mean/median reward -3.8/-6.0, min/max reward -10.0/8.0, entropy 1.77162, value loss 0.01000, policy loss -0.00369\n",
      "Updates 10100, num timesteps 808080, FPS 926, mean/median reward -3.2/-4.0, min/max reward -10.0/8.0, entropy 1.77689, value loss 0.03784, policy loss 0.01259\n",
      "Updates 10200, num timesteps 816080, FPS 926, mean/median reward -3.2/-4.0, min/max reward -10.0/8.0, entropy 1.66208, value loss 0.07453, policy loss -0.02198\n",
      "Updates 10300, num timesteps 824080, FPS 926, mean/median reward -3.6/-4.0, min/max reward -10.0/8.0, entropy 1.64997, value loss 0.07123, policy loss -0.07980\n",
      "Updates 10400, num timesteps 832080, FPS 926, mean/median reward -3.8/-5.0, min/max reward -10.0/8.0, entropy 1.76504, value loss 0.03705, policy loss 0.13848\n",
      "Updates 10500, num timesteps 840080, FPS 926, mean/median reward -5.0/-5.0, min/max reward -13.0/2.0, entropy 1.75352, value loss 0.04308, policy loss -0.03583\n",
      "Updates 10600, num timesteps 848080, FPS 926, mean/median reward -3.9/-4.0, min/max reward -13.0/3.0, entropy 1.78488, value loss 0.02210, policy loss 0.04126\n",
      "Updates 10700, num timesteps 856080, FPS 926, mean/median reward -3.8/-4.0, min/max reward -13.0/3.0, entropy 1.74605, value loss 0.01128, policy loss -0.00712\n",
      "Updates 10800, num timesteps 864080, FPS 926, mean/median reward -3.5/-4.0, min/max reward -13.0/3.0, entropy 1.67595, value loss 0.03905, policy loss 0.02272\n",
      "Updates 10900, num timesteps 872080, FPS 926, mean/median reward -2.3/-3.0, min/max reward -13.0/6.0, entropy 1.75912, value loss 0.01685, policy loss -0.01026\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 11000, num timesteps 880080, FPS 926, mean/median reward 0.0/-1.0, min/max reward -6.0/6.0, entropy 1.77381, value loss 0.11138, policy loss 0.19794\n",
      "Updates 11100, num timesteps 888080, FPS 926, mean/median reward -0.8/-2.0, min/max reward -8.0/6.0, entropy 1.76131, value loss 0.00787, policy loss -0.01736\n",
      "Updates 11200, num timesteps 896080, FPS 926, mean/median reward -1.7/-3.0, min/max reward -8.0/6.0, entropy 1.75826, value loss 0.03277, policy loss -0.00363\n",
      "Updates 11300, num timesteps 904080, FPS 926, mean/median reward -1.4/-3.0, min/max reward -8.0/6.0, entropy 1.72811, value loss 0.00885, policy loss -0.00575\n",
      "Updates 11400, num timesteps 912080, FPS 926, mean/median reward -1.5/-3.0, min/max reward -8.0/6.0, entropy 1.78134, value loss 0.01324, policy loss -0.00269\n",
      "Updates 11500, num timesteps 920080, FPS 926, mean/median reward -3.0/-4.0, min/max reward -11.0/5.0, entropy 1.66347, value loss 0.07882, policy loss -0.05951\n",
      "Updates 11600, num timesteps 928080, FPS 926, mean/median reward -4.6/-5.0, min/max reward -11.0/4.0, entropy 1.75386, value loss 0.00863, policy loss 0.03277\n",
      "Updates 11700, num timesteps 936080, FPS 926, mean/median reward -3.5/-5.0, min/max reward -11.0/5.0, entropy 1.68828, value loss 0.06153, policy loss 0.06742\n",
      "Updates 11800, num timesteps 944080, FPS 926, mean/median reward -2.2/-4.0, min/max reward -11.0/6.0, entropy 1.78006, value loss 0.02096, policy loss 0.01571\n",
      "Updates 11900, num timesteps 952080, FPS 926, mean/median reward -2.3/-4.0, min/max reward -11.0/6.0, entropy 1.78127, value loss 0.03365, policy loss 0.06854\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 12000, num timesteps 960080, FPS 926, mean/median reward -2.9/-5.0, min/max reward -11.0/6.0, entropy 1.73357, value loss 0.07197, policy loss -0.10713\n",
      "Updates 12100, num timesteps 968080, FPS 926, mean/median reward -1.2/-4.0, min/max reward -9.0/6.0, entropy 1.76085, value loss 0.02414, policy loss 0.00009\n",
      "Updates 12200, num timesteps 976080, FPS 926, mean/median reward -1.0/-3.0, min/max reward -9.0/6.0, entropy 1.72065, value loss 0.05378, policy loss -0.17293\n",
      "Updates 12300, num timesteps 984080, FPS 926, mean/median reward -0.1/1.0, min/max reward -9.0/6.0, entropy 1.65414, value loss 0.05556, policy loss -0.02375\n",
      "Updates 12400, num timesteps 992080, FPS 926, mean/median reward -0.2/1.0, min/max reward -9.0/6.0, entropy 1.75274, value loss 0.01795, policy loss 0.04561\n",
      "Updates 12500, num timesteps 1000080, FPS 926, mean/median reward 1.4/2.0, min/max reward -9.0/9.0, entropy 1.74757, value loss 0.02670, policy loss -0.07802\n",
      "Updates 12600, num timesteps 1008080, FPS 926, mean/median reward 1.6/3.0, min/max reward -9.0/9.0, entropy 1.76907, value loss 0.02559, policy loss 0.02884\n",
      "Updates 12700, num timesteps 1016080, FPS 926, mean/median reward 2.9/3.0, min/max reward -9.0/14.0, entropy 1.75343, value loss 0.08216, policy loss -0.24781\n",
      "Updates 12800, num timesteps 1024080, FPS 926, mean/median reward 2.9/3.0, min/max reward -9.0/14.0, entropy 1.73902, value loss 0.01172, policy loss 0.01004\n",
      "Updates 12900, num timesteps 1032080, FPS 926, mean/median reward 1.6/3.0, min/max reward -9.0/14.0, entropy 1.74554, value loss 0.01147, policy loss -0.03123\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 13000, num timesteps 1040080, FPS 926, mean/median reward 0.6/-1.0, min/max reward -9.0/14.0, entropy 1.77000, value loss 0.03043, policy loss -0.06566\n",
      "Updates 13100, num timesteps 1048080, FPS 926, mean/median reward 0.3/-2.0, min/max reward -9.0/14.0, entropy 1.72243, value loss 0.03742, policy loss -0.02464\n",
      "Updates 13200, num timesteps 1056080, FPS 926, mean/median reward 0.7/-1.0, min/max reward -9.0/14.0, entropy 1.76716, value loss 0.02357, policy loss 0.06130\n",
      "Updates 13300, num timesteps 1064080, FPS 926, mean/median reward 0.8/-1.0, min/max reward -9.0/14.0, entropy 1.72544, value loss 0.05460, policy loss -0.04797\n",
      "Updates 13400, num timesteps 1072080, FPS 926, mean/median reward 2.3/3.0, min/max reward -9.0/10.0, entropy 1.68125, value loss 0.02680, policy loss 0.01217\n",
      "Updates 13500, num timesteps 1080080, FPS 926, mean/median reward 2.9/3.0, min/max reward -9.0/10.0, entropy 1.77647, value loss 0.01510, policy loss 0.04257\n",
      "Updates 13600, num timesteps 1088080, FPS 926, mean/median reward 4.6/4.0, min/max reward -2.0/10.0, entropy 1.72965, value loss 0.01591, policy loss 0.00349\n",
      "Updates 13700, num timesteps 1096080, FPS 926, mean/median reward 5.4/4.0, min/max reward -2.0/13.0, entropy 1.73834, value loss 0.05435, policy loss -0.03918\n",
      "Updates 13800, num timesteps 1104080, FPS 926, mean/median reward 6.1/7.0, min/max reward -2.0/13.0, entropy 1.72873, value loss 0.06799, policy loss -0.10445\n",
      "Updates 13900, num timesteps 1112080, FPS 926, mean/median reward 7.4/9.0, min/max reward -2.0/13.0, entropy 1.72419, value loss 0.02220, policy loss -0.06213\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 14000, num timesteps 1120080, FPS 926, mean/median reward 7.8/9.0, min/max reward -2.0/13.0, entropy 1.77652, value loss 0.01028, policy loss 0.01049\n",
      "Updates 14100, num timesteps 1128080, FPS 926, mean/median reward 8.4/9.0, min/max reward -2.0/13.0, entropy 1.78451, value loss 0.02582, policy loss -0.03630\n",
      "Updates 14200, num timesteps 1136080, FPS 926, mean/median reward 7.7/9.0, min/max reward -3.0/13.0, entropy 1.72727, value loss 0.02870, policy loss 0.05660\n",
      "Updates 14300, num timesteps 1144080, FPS 926, mean/median reward 7.7/9.0, min/max reward -3.0/13.0, entropy 1.73606, value loss 0.01792, policy loss -0.07246\n",
      "Updates 14400, num timesteps 1152080, FPS 926, mean/median reward 7.4/9.0, min/max reward -3.0/13.0, entropy 1.78588, value loss 0.00550, policy loss 0.00477\n",
      "Updates 14500, num timesteps 1160080, FPS 926, mean/median reward 5.4/5.0, min/max reward -3.0/13.0, entropy 1.74983, value loss 0.02243, policy loss -0.06791\n",
      "Updates 14600, num timesteps 1168080, FPS 926, mean/median reward 3.9/3.0, min/max reward -3.0/12.0, entropy 1.77471, value loss 0.01582, policy loss 0.04543\n",
      "Updates 14700, num timesteps 1176080, FPS 926, mean/median reward 3.0/2.0, min/max reward -3.0/12.0, entropy 1.70538, value loss 0.04036, policy loss 0.00236\n",
      "Updates 14800, num timesteps 1184080, FPS 926, mean/median reward 2.9/4.0, min/max reward -3.0/9.0, entropy 1.74194, value loss 0.02754, policy loss -0.08695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 14900, num timesteps 1192080, FPS 926, mean/median reward 3.4/4.0, min/max reward -3.0/12.0, entropy 1.76911, value loss 0.01287, policy loss 0.04571\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 15000, num timesteps 1200080, FPS 926, mean/median reward 2.9/4.0, min/max reward -3.0/12.0, entropy 1.78801, value loss 0.00867, policy loss 0.04264\n",
      "Updates 15100, num timesteps 1208080, FPS 926, mean/median reward 4.4/5.0, min/max reward -3.0/12.0, entropy 1.74763, value loss 0.00829, policy loss 0.01315\n",
      "Updates 15200, num timesteps 1216080, FPS 925, mean/median reward 4.8/5.0, min/max reward -2.0/12.0, entropy 1.77007, value loss 0.02806, policy loss -0.07889\n",
      "Updates 15300, num timesteps 1224080, FPS 925, mean/median reward 5.2/5.0, min/max reward -2.0/12.0, entropy 1.74990, value loss 0.01536, policy loss -0.03107\n",
      "Updates 15400, num timesteps 1232080, FPS 925, mean/median reward 5.0/5.0, min/max reward -2.0/12.0, entropy 1.77390, value loss 0.01966, policy loss 0.01615\n",
      "Updates 15500, num timesteps 1240080, FPS 925, mean/median reward 4.8/5.0, min/max reward -2.0/12.0, entropy 1.74157, value loss 0.01289, policy loss -0.00203\n",
      "Updates 15600, num timesteps 1248080, FPS 925, mean/median reward 4.3/5.0, min/max reward -2.0/12.0, entropy 1.77124, value loss 0.01337, policy loss 0.01011\n",
      "Updates 15700, num timesteps 1256080, FPS 925, mean/median reward 4.5/4.0, min/max reward -2.0/13.0, entropy 1.77586, value loss 0.02173, policy loss 0.01233\n",
      "Updates 15800, num timesteps 1264080, FPS 924, mean/median reward 4.6/5.0, min/max reward -2.0/13.0, entropy 1.74678, value loss 0.01402, policy loss -0.00989\n",
      "Updates 15900, num timesteps 1272080, FPS 924, mean/median reward 4.6/5.0, min/max reward -2.0/13.0, entropy 1.70753, value loss 0.02521, policy loss -0.02438\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 16000, num timesteps 1280080, FPS 924, mean/median reward 5.2/6.0, min/max reward -2.0/13.0, entropy 1.73167, value loss 0.01603, policy loss -0.07074\n",
      "Updates 16100, num timesteps 1288080, FPS 924, mean/median reward 4.8/5.0, min/max reward -2.0/13.0, entropy 1.69838, value loss 0.02999, policy loss 0.02223\n",
      "Updates 16200, num timesteps 1296080, FPS 924, mean/median reward 4.8/5.0, min/max reward -3.0/13.0, entropy 1.78083, value loss 0.01197, policy loss 0.01078\n",
      "Updates 16300, num timesteps 1304080, FPS 924, mean/median reward 4.2/5.0, min/max reward -11.0/13.0, entropy 1.73077, value loss 0.01349, policy loss -0.01962\n",
      "Updates 16400, num timesteps 1312080, FPS 924, mean/median reward 3.2/3.0, min/max reward -11.0/11.0, entropy 1.72478, value loss 0.02226, policy loss 0.05835\n",
      "Updates 16500, num timesteps 1320080, FPS 923, mean/median reward 3.2/3.0, min/max reward -11.0/11.0, entropy 1.72827, value loss 0.03155, policy loss 0.01491\n",
      "Updates 16600, num timesteps 1328080, FPS 923, mean/median reward 4.1/5.0, min/max reward -11.0/14.0, entropy 1.75479, value loss 0.03346, policy loss -0.06599\n",
      "Updates 16700, num timesteps 1336080, FPS 923, mean/median reward 6.2/8.0, min/max reward -11.0/16.0, entropy 1.77232, value loss 0.01037, policy loss -0.04272\n",
      "Updates 16800, num timesteps 1344080, FPS 923, mean/median reward 6.3/7.0, min/max reward -11.0/16.0, entropy 1.68349, value loss 0.02469, policy loss 0.08485\n",
      "Updates 16900, num timesteps 1352080, FPS 923, mean/median reward 8.4/7.0, min/max reward 4.0/16.0, entropy 1.75515, value loss 0.04992, policy loss -0.05840\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 17000, num timesteps 1360080, FPS 923, mean/median reward 8.4/7.0, min/max reward 4.0/16.0, entropy 1.75731, value loss 0.02072, policy loss 0.01792\n",
      "Updates 17100, num timesteps 1368080, FPS 923, mean/median reward 8.2/7.0, min/max reward 4.0/16.0, entropy 1.63728, value loss 0.01258, policy loss 0.03929\n",
      "Updates 17200, num timesteps 1376080, FPS 922, mean/median reward 7.2/6.0, min/max reward 3.0/13.0, entropy 1.67604, value loss 0.11245, policy loss -0.14212\n",
      "Updates 17300, num timesteps 1384080, FPS 922, mean/median reward 7.6/6.0, min/max reward 1.0/14.0, entropy 1.77808, value loss 0.00681, policy loss 0.00049\n",
      "Updates 17400, num timesteps 1392080, FPS 922, mean/median reward 8.9/9.0, min/max reward 1.0/16.0, entropy 1.70799, value loss 0.04658, policy loss -0.05405\n",
      "Updates 17500, num timesteps 1400080, FPS 922, mean/median reward 9.4/9.0, min/max reward 1.0/16.0, entropy 1.75445, value loss 0.00580, policy loss -0.03752\n",
      "Updates 17600, num timesteps 1408080, FPS 922, mean/median reward 9.4/9.0, min/max reward 1.0/16.0, entropy 1.70320, value loss 0.01998, policy loss 0.06760\n",
      "Updates 17700, num timesteps 1416080, FPS 922, mean/median reward 9.1/9.0, min/max reward -9.0/19.0, entropy 1.76657, value loss 0.01542, policy loss 0.06463\n",
      "Updates 17800, num timesteps 1424080, FPS 922, mean/median reward 8.6/9.0, min/max reward -9.0/19.0, entropy 1.72150, value loss 0.03024, policy loss 0.02020\n",
      "Updates 17900, num timesteps 1432080, FPS 922, mean/median reward 9.0/9.0, min/max reward -9.0/19.0, entropy 1.78452, value loss 0.01962, policy loss -0.03555\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 18000, num timesteps 1440080, FPS 922, mean/median reward 8.6/9.0, min/max reward -9.0/19.0, entropy 1.75159, value loss 0.00986, policy loss -0.01295\n",
      "Updates 18100, num timesteps 1448080, FPS 921, mean/median reward 9.0/9.0, min/max reward -9.0/19.0, entropy 1.74117, value loss 0.01949, policy loss -0.00494\n",
      "Updates 18200, num timesteps 1456080, FPS 921, mean/median reward 7.7/8.0, min/max reward -9.0/16.0, entropy 1.75381, value loss 0.01248, policy loss 0.02436\n",
      "Updates 18300, num timesteps 1464080, FPS 921, mean/median reward 9.4/9.0, min/max reward 1.0/16.0, entropy 1.66274, value loss 0.04313, policy loss 0.14022\n",
      "Updates 18400, num timesteps 1472080, FPS 921, mean/median reward 10.6/11.0, min/max reward 1.0/16.0, entropy 1.69467, value loss 0.01847, policy loss -0.03011\n",
      "Updates 18500, num timesteps 1480080, FPS 921, mean/median reward 11.2/12.0, min/max reward 1.0/16.0, entropy 1.73658, value loss 0.00591, policy loss -0.02205\n",
      "Updates 18600, num timesteps 1488080, FPS 921, mean/median reward 11.8/12.0, min/max reward 1.0/16.0, entropy 1.78394, value loss 0.01046, policy loss 0.03728\n",
      "Updates 18700, num timesteps 1496080, FPS 921, mean/median reward 13.2/13.0, min/max reward 7.0/16.0, entropy 1.69440, value loss 0.02768, policy loss 0.08331\n",
      "Updates 18800, num timesteps 1504080, FPS 921, mean/median reward 12.3/12.0, min/max reward 7.0/16.0, entropy 1.69940, value loss 0.01606, policy loss -0.00856\n",
      "Updates 18900, num timesteps 1512080, FPS 921, mean/median reward 11.5/11.0, min/max reward 7.0/16.0, entropy 1.76164, value loss 0.07824, policy loss 0.00774\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 19000, num timesteps 1520080, FPS 920, mean/median reward 10.2/10.0, min/max reward -3.0/16.0, entropy 1.66981, value loss 0.06010, policy loss -0.08900\n",
      "Updates 19100, num timesteps 1528080, FPS 920, mean/median reward 9.8/10.0, min/max reward -3.0/16.0, entropy 1.64088, value loss 0.02238, policy loss 0.07460\n",
      "Updates 19200, num timesteps 1536080, FPS 920, mean/median reward 9.6/10.0, min/max reward -3.0/16.0, entropy 1.63713, value loss 0.01728, policy loss -0.02500\n",
      "Updates 19300, num timesteps 1544080, FPS 920, mean/median reward 9.9/10.0, min/max reward -3.0/16.0, entropy 1.73783, value loss 0.08713, policy loss -0.13113\n",
      "Updates 19400, num timesteps 1552080, FPS 920, mean/median reward 9.0/10.0, min/max reward -3.0/16.0, entropy 1.64752, value loss 0.00918, policy loss -0.01623\n",
      "Updates 19500, num timesteps 1560080, FPS 920, mean/median reward 9.2/11.0, min/max reward 2.0/16.0, entropy 1.70072, value loss 0.02654, policy loss -0.05203\n",
      "Updates 19600, num timesteps 1568080, FPS 920, mean/median reward 9.4/11.0, min/max reward 2.0/16.0, entropy 1.70893, value loss 0.01110, policy loss 0.00309\n",
      "Updates 19700, num timesteps 1576080, FPS 920, mean/median reward 9.0/10.0, min/max reward 2.0/14.0, entropy 1.77444, value loss 0.01198, policy loss 0.02532\n",
      "Updates 19800, num timesteps 1584080, FPS 920, mean/median reward 7.2/10.0, min/max reward -10.0/14.0, entropy 1.78390, value loss 0.01180, policy loss 0.00934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19900, num timesteps 1592080, FPS 920, mean/median reward 6.9/8.0, min/max reward -10.0/14.0, entropy 1.71690, value loss 0.02310, policy loss 0.00222\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 20000, num timesteps 1600080, FPS 920, mean/median reward 6.1/6.0, min/max reward -10.0/14.0, entropy 1.62491, value loss 0.16029, policy loss -0.08779\n",
      "Updates 20100, num timesteps 1608080, FPS 920, mean/median reward 5.0/6.0, min/max reward -10.0/14.0, entropy 1.76196, value loss 0.01170, policy loss -0.00121\n",
      "Updates 20200, num timesteps 1616080, FPS 920, mean/median reward 5.2/6.0, min/max reward -10.0/14.0, entropy 1.54789, value loss 0.02881, policy loss 0.02382\n",
      "Updates 20300, num timesteps 1624080, FPS 919, mean/median reward 7.5/7.0, min/max reward 1.0/14.0, entropy 1.61150, value loss 0.10932, policy loss -0.11830\n",
      "Updates 20400, num timesteps 1632080, FPS 919, mean/median reward 9.3/10.0, min/max reward 1.0/17.0, entropy 1.64219, value loss 0.03291, policy loss -0.05073\n",
      "Updates 20500, num timesteps 1640080, FPS 919, mean/median reward 9.8/11.0, min/max reward 1.0/17.0, entropy 1.67816, value loss 0.02447, policy loss -0.01365\n",
      "Updates 20600, num timesteps 1648080, FPS 919, mean/median reward 10.2/11.0, min/max reward 1.0/17.0, entropy 1.71987, value loss 0.01434, policy loss 0.02695\n",
      "Updates 20700, num timesteps 1656080, FPS 919, mean/median reward 11.4/11.0, min/max reward 5.0/17.0, entropy 1.74235, value loss 0.00970, policy loss 0.00551\n",
      "Updates 20800, num timesteps 1664080, FPS 919, mean/median reward 11.4/11.0, min/max reward 5.0/17.0, entropy 1.69747, value loss 0.09070, policy loss -0.18594\n",
      "Updates 20900, num timesteps 1672080, FPS 919, mean/median reward 10.2/11.0, min/max reward 5.0/14.0, entropy 1.67174, value loss 0.02303, policy loss -0.07825\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 21000, num timesteps 1680080, FPS 919, mean/median reward 10.2/11.0, min/max reward 5.0/14.0, entropy 1.73561, value loss 0.01398, policy loss -0.03292\n",
      "Updates 21100, num timesteps 1688080, FPS 919, mean/median reward 10.6/11.0, min/max reward 6.0/14.0, entropy 1.77896, value loss 0.01011, policy loss 0.02670\n",
      "Updates 21200, num timesteps 1696080, FPS 919, mean/median reward 10.2/11.0, min/max reward 4.0/14.0, entropy 1.67090, value loss 0.01238, policy loss 0.00633\n",
      "Updates 21300, num timesteps 1704080, FPS 919, mean/median reward 9.8/11.0, min/max reward 3.0/14.0, entropy 1.74545, value loss 0.00971, policy loss -0.03212\n",
      "Updates 21400, num timesteps 1712080, FPS 919, mean/median reward 8.6/8.0, min/max reward 1.0/18.0, entropy 1.62421, value loss 0.02034, policy loss -0.03011\n",
      "Updates 21500, num timesteps 1720080, FPS 918, mean/median reward 8.2/7.0, min/max reward 1.0/18.0, entropy 1.74729, value loss 0.02432, policy loss 0.02912\n",
      "Updates 21600, num timesteps 1728080, FPS 918, mean/median reward 9.3/8.0, min/max reward 1.0/18.0, entropy 1.66660, value loss 0.01742, policy loss 0.01433\n",
      "Updates 21700, num timesteps 1736080, FPS 918, mean/median reward 10.8/11.0, min/max reward 1.0/18.0, entropy 1.64970, value loss 0.01309, policy loss 0.04454\n",
      "Updates 21800, num timesteps 1744080, FPS 918, mean/median reward 12.6/12.0, min/max reward 1.0/20.0, entropy 1.70074, value loss 0.01213, policy loss -0.02972\n",
      "Updates 21900, num timesteps 1752080, FPS 918, mean/median reward 14.3/15.0, min/max reward 9.0/20.0, entropy 1.75059, value loss 0.00694, policy loss 0.01060\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 22000, num timesteps 1760080, FPS 918, mean/median reward 14.3/15.0, min/max reward 9.0/20.0, entropy 1.71336, value loss 0.04149, policy loss -0.01484\n",
      "Updates 22100, num timesteps 1768080, FPS 918, mean/median reward 14.1/13.0, min/max reward 9.0/20.0, entropy 1.66104, value loss 0.02474, policy loss 0.00859\n",
      "Updates 22200, num timesteps 1776080, FPS 918, mean/median reward 14.2/13.0, min/max reward 10.0/18.0, entropy 1.70852, value loss 0.02506, policy loss -0.05150\n",
      "Updates 22300, num timesteps 1784080, FPS 918, mean/median reward 13.2/12.0, min/max reward 7.0/18.0, entropy 1.73885, value loss 0.01798, policy loss -0.05546\n",
      "Updates 22400, num timesteps 1792080, FPS 918, mean/median reward 12.8/12.0, min/max reward 7.0/18.0, entropy 1.75802, value loss 0.00685, policy loss 0.01389\n",
      "Updates 22500, num timesteps 1800080, FPS 918, mean/median reward 13.2/12.0, min/max reward 7.0/19.0, entropy 1.73187, value loss 0.00559, policy loss 0.04048\n",
      "Updates 22600, num timesteps 1808080, FPS 918, mean/median reward 13.5/13.0, min/max reward 7.0/19.0, entropy 1.69113, value loss 0.03371, policy loss -0.03936\n",
      "Updates 22700, num timesteps 1816080, FPS 918, mean/median reward 14.9/14.0, min/max reward 10.0/19.0, entropy 1.76940, value loss 0.00521, policy loss -0.02187\n",
      "Updates 22800, num timesteps 1824080, FPS 918, mean/median reward 15.6/16.0, min/max reward 12.0/19.0, entropy 1.71494, value loss 0.02518, policy loss -0.08082\n",
      "Updates 22900, num timesteps 1832080, FPS 917, mean/median reward 15.0/14.0, min/max reward 10.0/19.0, entropy 1.71147, value loss 0.02472, policy loss 0.03619\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 23000, num timesteps 1840080, FPS 917, mean/median reward 13.8/14.0, min/max reward 6.0/19.0, entropy 1.62232, value loss 0.03579, policy loss -0.03993\n",
      "Updates 23100, num timesteps 1848080, FPS 917, mean/median reward 12.8/13.0, min/max reward 6.0/19.0, entropy 1.77120, value loss 0.03745, policy loss 0.02692\n",
      "Updates 23200, num timesteps 1856080, FPS 917, mean/median reward 10.9/11.0, min/max reward 6.0/14.0, entropy 1.66457, value loss 0.05145, policy loss -0.02867\n",
      "Updates 23300, num timesteps 1864080, FPS 917, mean/median reward 10.8/11.0, min/max reward 6.0/14.0, entropy 1.74017, value loss 0.04381, policy loss 0.10522\n",
      "Updates 23400, num timesteps 1872080, FPS 917, mean/median reward 10.9/12.0, min/max reward 6.0/14.0, entropy 1.69411, value loss 0.01902, policy loss 0.00367\n",
      "Updates 23500, num timesteps 1880080, FPS 917, mean/median reward 11.6/12.0, min/max reward 7.0/15.0, entropy 1.72731, value loss 0.02540, policy loss -0.03320\n",
      "Updates 23600, num timesteps 1888080, FPS 917, mean/median reward 10.9/11.0, min/max reward 7.0/15.0, entropy 1.70840, value loss 0.01521, policy loss -0.00294\n",
      "Updates 23700, num timesteps 1896080, FPS 917, mean/median reward 10.1/10.0, min/max reward 5.0/15.0, entropy 1.71604, value loss 0.00909, policy loss -0.00747\n",
      "Updates 23800, num timesteps 1904080, FPS 917, mean/median reward 9.8/10.0, min/max reward 5.0/15.0, entropy 1.61595, value loss 0.01653, policy loss -0.02165\n",
      "Updates 23900, num timesteps 1912080, FPS 917, mean/median reward 10.6/10.0, min/max reward 2.0/18.0, entropy 1.57047, value loss 0.02209, policy loss -0.01271\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 24000, num timesteps 1920080, FPS 917, mean/median reward 11.2/11.0, min/max reward 2.0/18.0, entropy 1.66844, value loss 0.01418, policy loss -0.00634\n",
      "Updates 24100, num timesteps 1928080, FPS 917, mean/median reward 13.6/15.0, min/max reward 2.0/18.0, entropy 1.72882, value loss 0.01598, policy loss 0.03076\n",
      "Updates 24200, num timesteps 1936080, FPS 917, mean/median reward 14.0/15.0, min/max reward 2.0/18.0, entropy 1.73671, value loss 0.02606, policy loss 0.06458\n",
      "Updates 24300, num timesteps 1944080, FPS 916, mean/median reward 14.0/15.0, min/max reward 2.0/18.0, entropy 1.75610, value loss 0.03147, policy loss -0.00482\n",
      "Updates 24400, num timesteps 1952080, FPS 916, mean/median reward 13.0/13.0, min/max reward 6.0/19.0, entropy 1.65224, value loss 0.01647, policy loss -0.01437\n",
      "Updates 24500, num timesteps 1960080, FPS 916, mean/median reward 12.0/13.0, min/max reward -2.0/19.0, entropy 1.70139, value loss 0.01997, policy loss 0.02900\n",
      "Updates 24600, num timesteps 1968080, FPS 916, mean/median reward 11.4/12.0, min/max reward -2.0/19.0, entropy 1.76806, value loss 0.04904, policy loss 0.08147\n",
      "Updates 24700, num timesteps 1976080, FPS 916, mean/median reward 10.6/10.0, min/max reward -2.0/19.0, entropy 1.66544, value loss 0.00624, policy loss 0.03262\n",
      "Updates 24800, num timesteps 1984080, FPS 916, mean/median reward 12.1/14.0, min/max reward -2.0/18.0, entropy 1.76352, value loss 0.00736, policy loss -0.01028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 24900, num timesteps 1992080, FPS 916, mean/median reward 13.6/14.0, min/max reward 3.0/19.0, entropy 1.72175, value loss 0.00424, policy loss 0.01807\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 25000, num timesteps 2000080, FPS 916, mean/median reward 14.0/14.0, min/max reward 3.0/19.0, entropy 1.70339, value loss 0.00753, policy loss -0.01848\n",
      "Updates 25100, num timesteps 2008080, FPS 916, mean/median reward 14.0/14.0, min/max reward 3.0/19.0, entropy 1.77517, value loss 0.00867, policy loss -0.00310\n",
      "Updates 25200, num timesteps 2016080, FPS 916, mean/median reward 14.3/15.0, min/max reward 3.0/19.0, entropy 1.74387, value loss 0.02335, policy loss 0.05952\n",
      "Updates 25300, num timesteps 2024080, FPS 916, mean/median reward 14.1/14.0, min/max reward 7.0/18.0, entropy 1.73640, value loss 0.01612, policy loss -0.01772\n",
      "Updates 25400, num timesteps 2032080, FPS 916, mean/median reward 14.3/15.0, min/max reward 7.0/18.0, entropy 1.62049, value loss 0.00741, policy loss 0.03633\n",
      "Updates 25500, num timesteps 2040080, FPS 916, mean/median reward 14.4/15.0, min/max reward 7.0/18.0, entropy 1.77201, value loss 0.01528, policy loss -0.02227\n",
      "Updates 25600, num timesteps 2048080, FPS 916, mean/median reward 14.4/15.0, min/max reward 7.0/18.0, entropy 1.74502, value loss 0.02001, policy loss -0.01630\n",
      "Updates 25700, num timesteps 2056080, FPS 916, mean/median reward 13.7/15.0, min/max reward 7.0/18.0, entropy 1.77786, value loss 0.00770, policy loss -0.01037\n",
      "Updates 25800, num timesteps 2064080, FPS 916, mean/median reward 13.7/14.0, min/max reward 8.0/18.0, entropy 1.77271, value loss 0.01079, policy loss 0.08875\n",
      "Updates 25900, num timesteps 2072080, FPS 916, mean/median reward 13.4/14.0, min/max reward 7.0/18.0, entropy 1.67611, value loss 0.01302, policy loss -0.01003\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 26000, num timesteps 2080080, FPS 915, mean/median reward 13.4/14.0, min/max reward 7.0/18.0, entropy 1.73509, value loss 0.00326, policy loss 0.02867\n",
      "Updates 26100, num timesteps 2088080, FPS 915, mean/median reward 13.8/14.0, min/max reward 7.0/18.0, entropy 1.74097, value loss 0.04020, policy loss -0.01330\n",
      "Updates 26200, num timesteps 2096080, FPS 915, mean/median reward 14.5/15.0, min/max reward 7.0/19.0, entropy 1.74021, value loss 0.01931, policy loss 0.04396\n",
      "Updates 26300, num timesteps 2104080, FPS 915, mean/median reward 10.3/12.0, min/max reward -7.0/19.0, entropy 1.77427, value loss 0.01692, policy loss -0.08506\n",
      "Updates 26400, num timesteps 2112080, FPS 915, mean/median reward 8.9/8.0, min/max reward -7.0/19.0, entropy 1.74325, value loss 0.01550, policy loss -0.03976\n",
      "Updates 26500, num timesteps 2120080, FPS 915, mean/median reward 8.1/7.0, min/max reward -7.0/19.0, entropy 1.70463, value loss 0.01083, policy loss -0.01836\n",
      "Updates 26600, num timesteps 2128080, FPS 915, mean/median reward 6.8/7.0, min/max reward -7.0/15.0, entropy 1.77929, value loss 0.01581, policy loss 0.06081\n",
      "Updates 26700, num timesteps 2136080, FPS 915, mean/median reward 9.1/8.0, min/max reward -7.0/17.0, entropy 1.73575, value loss 0.01750, policy loss -0.06449\n",
      "Updates 26800, num timesteps 2144080, FPS 915, mean/median reward 12.1/14.0, min/max reward 4.0/17.0, entropy 1.72538, value loss 0.01499, policy loss -0.01890\n",
      "Updates 26900, num timesteps 2152080, FPS 915, mean/median reward 12.9/15.0, min/max reward 4.0/17.0, entropy 1.71382, value loss 0.05890, policy loss -0.10128\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 27000, num timesteps 2160080, FPS 915, mean/median reward 14.4/15.0, min/max reward 4.0/18.0, entropy 1.75108, value loss 0.00800, policy loss -0.00033\n",
      "Updates 27100, num timesteps 2168080, FPS 915, mean/median reward 15.2/15.0, min/max reward 7.0/19.0, entropy 1.73517, value loss 0.09803, policy loss -0.09646\n",
      "Updates 27200, num timesteps 2176080, FPS 915, mean/median reward 14.7/15.0, min/max reward 6.0/19.0, entropy 1.70114, value loss 0.01450, policy loss 0.00888\n",
      "Updates 27300, num timesteps 2184080, FPS 915, mean/median reward 14.1/15.0, min/max reward 6.0/19.0, entropy 1.75604, value loss 0.00450, policy loss 0.01012\n",
      "Updates 27400, num timesteps 2192080, FPS 915, mean/median reward 13.8/14.0, min/max reward 6.0/19.0, entropy 1.59791, value loss 0.03097, policy loss -0.11548\n",
      "Updates 27500, num timesteps 2200080, FPS 915, mean/median reward 11.9/13.0, min/max reward 1.0/20.0, entropy 1.70392, value loss 0.01628, policy loss 0.05186\n",
      "Updates 27600, num timesteps 2208080, FPS 915, mean/median reward 12.3/13.0, min/max reward 1.0/20.0, entropy 1.66183, value loss 0.01537, policy loss -0.00877\n",
      "Updates 27700, num timesteps 2216080, FPS 915, mean/median reward 13.1/13.0, min/max reward 1.0/20.0, entropy 1.78296, value loss 0.04735, policy loss 0.03132\n",
      "Updates 27800, num timesteps 2224080, FPS 915, mean/median reward 13.1/13.0, min/max reward 1.0/20.0, entropy 1.71527, value loss 0.04689, policy loss 0.06020\n",
      "Updates 27900, num timesteps 2232080, FPS 915, mean/median reward 9.9/12.0, min/max reward -9.0/20.0, entropy 1.73442, value loss 0.01307, policy loss -0.06521\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 28000, num timesteps 2240080, FPS 914, mean/median reward 6.1/8.0, min/max reward -9.0/19.0, entropy 1.67898, value loss 0.01500, policy loss 0.02404\n",
      "Updates 28100, num timesteps 2248080, FPS 914, mean/median reward 5.2/4.0, min/max reward -9.0/19.0, entropy 1.77582, value loss 0.01101, policy loss -0.03547\n",
      "Updates 28200, num timesteps 2256080, FPS 914, mean/median reward 2.4/3.0, min/max reward -9.0/18.0, entropy 1.74284, value loss 0.01417, policy loss 0.02711\n",
      "Updates 28300, num timesteps 2264080, FPS 914, mean/median reward 2.4/3.0, min/max reward -9.0/18.0, entropy 1.76358, value loss 0.00271, policy loss 0.00854\n",
      "Updates 28400, num timesteps 2272080, FPS 914, mean/median reward 4.7/4.0, min/max reward -5.0/18.0, entropy 1.59683, value loss 0.01609, policy loss 0.04832\n",
      "Updates 28500, num timesteps 2280080, FPS 914, mean/median reward 5.8/4.0, min/max reward -5.0/18.0, entropy 1.77643, value loss 0.02723, policy loss 0.07081\n",
      "Updates 28600, num timesteps 2288080, FPS 914, mean/median reward 7.5/6.0, min/max reward 3.0/18.0, entropy 1.78120, value loss 0.03201, policy loss -0.00471\n",
      "Updates 28700, num timesteps 2296080, FPS 914, mean/median reward 8.2/8.0, min/max reward 3.0/18.0, entropy 1.60294, value loss 0.04732, policy loss 0.08521\n",
      "Updates 28800, num timesteps 2304080, FPS 914, mean/median reward 7.7/8.0, min/max reward 2.0/12.0, entropy 1.72074, value loss 0.00844, policy loss 0.01343\n",
      "Updates 28900, num timesteps 2312080, FPS 914, mean/median reward 7.9/7.0, min/max reward 2.0/15.0, entropy 1.74212, value loss 0.04798, policy loss -0.10639\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 29000, num timesteps 2320080, FPS 914, mean/median reward 8.2/8.0, min/max reward 2.0/15.0, entropy 1.75544, value loss 0.02685, policy loss -0.06142\n",
      "Updates 29100, num timesteps 2328080, FPS 914, mean/median reward 8.4/8.0, min/max reward 2.0/15.0, entropy 1.73272, value loss 0.03074, policy loss 0.07933\n",
      "Updates 29200, num timesteps 2336080, FPS 914, mean/median reward 7.8/7.0, min/max reward -1.0/15.0, entropy 1.64734, value loss 0.01194, policy loss 0.00097\n",
      "Updates 29300, num timesteps 2344080, FPS 914, mean/median reward 8.1/8.0, min/max reward -1.0/15.0, entropy 1.77206, value loss 0.01808, policy loss -0.00275\n",
      "Updates 29400, num timesteps 2352080, FPS 914, mean/median reward 8.3/8.0, min/max reward -1.0/14.0, entropy 1.71920, value loss 0.02318, policy loss 0.03498\n",
      "Updates 29500, num timesteps 2360080, FPS 914, mean/median reward 7.5/6.0, min/max reward -1.0/14.0, entropy 1.73840, value loss 0.02110, policy loss 0.02621\n",
      "Updates 29600, num timesteps 2368080, FPS 914, mean/median reward 7.5/6.0, min/max reward -1.0/14.0, entropy 1.59092, value loss 0.09791, policy loss -0.08535\n",
      "Updates 29700, num timesteps 2376080, FPS 914, mean/median reward 9.2/7.0, min/max reward 3.0/16.0, entropy 1.65668, value loss 0.00560, policy loss 0.01808\n",
      "Updates 29800, num timesteps 2384080, FPS 914, mean/median reward 8.7/7.0, min/max reward 3.0/16.0, entropy 1.70073, value loss 0.02336, policy loss 0.01120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29900, num timesteps 2392080, FPS 914, mean/median reward 9.9/9.0, min/max reward 3.0/16.0, entropy 1.59564, value loss 0.05919, policy loss -0.04358\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 30000, num timesteps 2400080, FPS 914, mean/median reward 9.4/9.0, min/max reward 3.0/16.0, entropy 1.63768, value loss 0.01695, policy loss 0.04703\n",
      "Updates 30100, num timesteps 2408080, FPS 914, mean/median reward 10.3/10.0, min/max reward 3.0/16.0, entropy 1.69625, value loss 0.00704, policy loss 0.02764\n",
      "Updates 30200, num timesteps 2416080, FPS 913, mean/median reward 11.1/12.0, min/max reward 3.0/16.0, entropy 1.73658, value loss 0.01586, policy loss 0.02589\n",
      "Updates 30300, num timesteps 2424080, FPS 913, mean/median reward 10.2/10.0, min/max reward -2.0/16.0, entropy 1.70401, value loss 0.01936, policy loss 0.00575\n",
      "Updates 30400, num timesteps 2432080, FPS 913, mean/median reward 10.8/12.0, min/max reward -2.0/16.0, entropy 1.66382, value loss 0.00532, policy loss -0.02831\n",
      "Updates 30500, num timesteps 2440080, FPS 913, mean/median reward 11.6/13.0, min/max reward -2.0/18.0, entropy 1.76089, value loss 0.03827, policy loss 0.03533\n",
      "Updates 30600, num timesteps 2448080, FPS 913, mean/median reward 12.1/14.0, min/max reward -2.0/18.0, entropy 1.74764, value loss 0.01870, policy loss 0.01981\n",
      "Updates 30700, num timesteps 2456080, FPS 913, mean/median reward 12.6/14.0, min/max reward 2.0/18.0, entropy 1.76871, value loss 0.00999, policy loss 0.00282\n",
      "Updates 30800, num timesteps 2464080, FPS 913, mean/median reward 12.1/11.0, min/max reward 5.0/18.0, entropy 1.76555, value loss 0.04412, policy loss -0.09116\n",
      "Updates 30900, num timesteps 2472080, FPS 913, mean/median reward 7.9/9.0, min/max reward -8.0/17.0, entropy 1.73054, value loss 0.03801, policy loss -0.03194\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 31000, num timesteps 2480080, FPS 913, mean/median reward 5.6/8.0, min/max reward -10.0/17.0, entropy 1.76381, value loss 0.00383, policy loss -0.00200\n",
      "Updates 31100, num timesteps 2488080, FPS 913, mean/median reward 4.6/6.0, min/max reward -10.0/17.0, entropy 1.74077, value loss 0.02268, policy loss -0.03593\n",
      "Updates 31200, num timesteps 2496080, FPS 913, mean/median reward 1.9/4.0, min/max reward -10.0/17.0, entropy 1.76593, value loss 0.03523, policy loss -0.03773\n",
      "Updates 31300, num timesteps 2504080, FPS 913, mean/median reward 0.6/-2.0, min/max reward -10.0/10.0, entropy 1.72986, value loss 0.01408, policy loss -0.00885\n",
      "Updates 31400, num timesteps 2512080, FPS 913, mean/median reward 1.4/3.0, min/max reward -10.0/10.0, entropy 1.66881, value loss 0.01272, policy loss -0.03439\n",
      "Updates 31500, num timesteps 2520080, FPS 913, mean/median reward 1.7/3.0, min/max reward -10.0/10.0, entropy 1.75740, value loss 0.01282, policy loss -0.00039\n",
      "Updates 31600, num timesteps 2528080, FPS 913, mean/median reward 3.0/4.0, min/max reward -10.0/10.0, entropy 1.73998, value loss 0.04551, policy loss -0.06387\n",
      "Updates 31700, num timesteps 2536080, FPS 913, mean/median reward 6.8/8.0, min/max reward -4.0/13.0, entropy 1.72899, value loss 0.01551, policy loss -0.03501\n",
      "Updates 31800, num timesteps 2544080, FPS 913, mean/median reward 7.6/9.0, min/max reward -4.0/13.0, entropy 1.76095, value loss 0.02889, policy loss -0.00554\n",
      "Updates 31900, num timesteps 2552080, FPS 913, mean/median reward 7.1/9.0, min/max reward -7.0/13.0, entropy 1.72567, value loss 0.01443, policy loss 0.02764\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 32000, num timesteps 2560080, FPS 913, mean/median reward 6.2/8.0, min/max reward -7.0/13.0, entropy 1.57955, value loss 0.03127, policy loss -0.05735\n",
      "Updates 32100, num timesteps 2568080, FPS 913, mean/median reward 6.5/6.0, min/max reward -7.0/13.0, entropy 1.76411, value loss 0.01851, policy loss -0.00999\n",
      "Updates 32200, num timesteps 2576080, FPS 913, mean/median reward 6.0/6.0, min/max reward -7.0/13.0, entropy 1.67856, value loss 0.01314, policy loss 0.06376\n",
      "Updates 32300, num timesteps 2584080, FPS 913, mean/median reward 5.2/5.0, min/max reward -7.0/13.0, entropy 1.71529, value loss 0.06216, policy loss -0.03033\n",
      "Updates 32400, num timesteps 2592080, FPS 913, mean/median reward 7.2/6.0, min/max reward -5.0/18.0, entropy 1.71920, value loss 0.01385, policy loss -0.00345\n",
      "Updates 32500, num timesteps 2600080, FPS 913, mean/median reward 9.9/9.0, min/max reward 1.0/18.0, entropy 1.72631, value loss 0.01657, policy loss -0.05745\n",
      "Updates 32600, num timesteps 2608080, FPS 913, mean/median reward 11.6/11.0, min/max reward 1.0/19.0, entropy 1.74617, value loss 0.01621, policy loss -0.05650\n",
      "Updates 32700, num timesteps 2616080, FPS 914, mean/median reward 13.5/14.0, min/max reward 5.0/19.0, entropy 1.71523, value loss 0.00553, policy loss -0.00835\n",
      "Updates 32800, num timesteps 2624080, FPS 914, mean/median reward 13.9/14.0, min/max reward 7.0/19.0, entropy 1.75395, value loss 0.00454, policy loss -0.01025\n",
      "Updates 32900, num timesteps 2632080, FPS 914, mean/median reward 14.2/14.0, min/max reward 7.0/19.0, entropy 1.70673, value loss 0.03255, policy loss -0.04280\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 33000, num timesteps 2640080, FPS 914, mean/median reward 13.0/14.0, min/max reward 4.0/19.0, entropy 1.71455, value loss 0.01873, policy loss 0.03364\n",
      "Updates 33100, num timesteps 2648080, FPS 914, mean/median reward 12.8/13.0, min/max reward 4.0/19.0, entropy 1.69628, value loss 0.02605, policy loss -0.00107\n",
      "Updates 33200, num timesteps 2656080, FPS 914, mean/median reward 10.8/12.0, min/max reward 1.0/16.0, entropy 1.66843, value loss 0.03382, policy loss 0.01698\n",
      "Updates 33300, num timesteps 2664080, FPS 914, mean/median reward 10.4/12.0, min/max reward 1.0/16.0, entropy 1.75206, value loss 0.01154, policy loss -0.02244\n",
      "Updates 33400, num timesteps 2672080, FPS 914, mean/median reward 9.2/11.0, min/max reward 1.0/14.0, entropy 1.70923, value loss 0.02068, policy loss 0.03433\n",
      "Updates 33500, num timesteps 2680080, FPS 914, mean/median reward 9.8/11.0, min/max reward 1.0/15.0, entropy 1.76145, value loss 0.01562, policy loss 0.03174\n",
      "Updates 33600, num timesteps 2688080, FPS 914, mean/median reward 10.7/11.0, min/max reward 2.0/16.0, entropy 1.68535, value loss 0.02719, policy loss -0.02053\n",
      "Updates 33700, num timesteps 2696080, FPS 915, mean/median reward 10.7/11.0, min/max reward 2.0/16.0, entropy 1.69339, value loss 0.00789, policy loss 0.03019\n",
      "Updates 33800, num timesteps 2704080, FPS 915, mean/median reward 12.6/14.0, min/max reward 2.0/20.0, entropy 1.71015, value loss 0.01301, policy loss 0.04273\n",
      "Updates 33900, num timesteps 2712080, FPS 915, mean/median reward 13.6/14.0, min/max reward 8.0/20.0, entropy 1.64691, value loss 0.01161, policy loss -0.03987\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 34000, num timesteps 2720080, FPS 915, mean/median reward 14.2/14.0, min/max reward 8.0/20.0, entropy 1.72476, value loss 0.05112, policy loss -0.13477\n",
      "Updates 34100, num timesteps 2728080, FPS 915, mean/median reward 14.2/14.0, min/max reward 8.0/20.0, entropy 1.71652, value loss 0.01359, policy loss -0.00195\n",
      "Updates 34200, num timesteps 2736080, FPS 915, mean/median reward 14.6/15.0, min/max reward 9.0/20.0, entropy 1.76883, value loss 0.00871, policy loss -0.03297\n",
      "Updates 34300, num timesteps 2744080, FPS 915, mean/median reward 14.2/14.0, min/max reward 9.0/20.0, entropy 1.70917, value loss 0.09438, policy loss -0.03323\n",
      "Updates 34400, num timesteps 2752080, FPS 915, mean/median reward 13.8/15.0, min/max reward 8.0/19.0, entropy 1.73395, value loss 0.00473, policy loss -0.00064\n",
      "Updates 34500, num timesteps 2760080, FPS 915, mean/median reward 13.9/16.0, min/max reward 5.0/19.0, entropy 1.72391, value loss 0.02199, policy loss -0.00488\n",
      "Updates 34600, num timesteps 2768080, FPS 915, mean/median reward 13.2/14.0, min/max reward 5.0/19.0, entropy 1.64442, value loss 0.01249, policy loss -0.04799\n",
      "Updates 34700, num timesteps 2776080, FPS 915, mean/median reward 13.2/14.0, min/max reward 5.0/19.0, entropy 1.64491, value loss 0.02437, policy loss -0.10421\n",
      "Updates 34800, num timesteps 2784080, FPS 915, mean/median reward 12.9/12.0, min/max reward 5.0/20.0, entropy 1.74235, value loss 0.00616, policy loss -0.01905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 34900, num timesteps 2792080, FPS 915, mean/median reward 12.9/12.0, min/max reward 6.0/20.0, entropy 1.76887, value loss 0.00831, policy loss -0.04415\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 35000, num timesteps 2800080, FPS 916, mean/median reward 13.4/12.0, min/max reward 8.0/20.0, entropy 1.66570, value loss 0.01261, policy loss 0.02815\n",
      "Updates 35100, num timesteps 2808080, FPS 916, mean/median reward 13.0/12.0, min/max reward 7.0/20.0, entropy 1.72526, value loss 0.00701, policy loss 0.04530\n",
      "Updates 35200, num timesteps 2816080, FPS 916, mean/median reward 13.6/14.0, min/max reward 7.0/18.0, entropy 1.70166, value loss 0.00564, policy loss 0.05194\n",
      "Updates 35300, num timesteps 2824080, FPS 916, mean/median reward 12.6/12.0, min/max reward 7.0/18.0, entropy 1.49994, value loss 0.02040, policy loss 0.01479\n",
      "Updates 35400, num timesteps 2832080, FPS 916, mean/median reward 12.7/12.0, min/max reward 7.0/18.0, entropy 1.67105, value loss 0.03660, policy loss -0.01666\n",
      "Updates 35500, num timesteps 2840080, FPS 916, mean/median reward 11.8/11.0, min/max reward 3.0/18.0, entropy 1.72622, value loss 0.01316, policy loss -0.00262\n",
      "Updates 35600, num timesteps 2848080, FPS 916, mean/median reward 10.8/10.0, min/max reward -3.0/18.0, entropy 1.73046, value loss 0.01255, policy loss 0.07317\n",
      "Updates 35700, num timesteps 2856080, FPS 916, mean/median reward 9.6/10.0, min/max reward -3.0/18.0, entropy 1.75834, value loss 0.01865, policy loss -0.01950\n",
      "Updates 35800, num timesteps 2864080, FPS 916, mean/median reward 9.3/10.0, min/max reward -3.0/18.0, entropy 1.67008, value loss 0.04330, policy loss -0.02755\n",
      "Updates 35900, num timesteps 2872080, FPS 916, mean/median reward 9.3/10.0, min/max reward -3.0/18.0, entropy 1.62541, value loss 0.01741, policy loss 0.04077\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 36000, num timesteps 2880080, FPS 916, mean/median reward 8.5/8.0, min/max reward -3.0/18.0, entropy 1.73315, value loss 0.04487, policy loss -0.08123\n",
      "Updates 36100, num timesteps 2888080, FPS 916, mean/median reward 8.2/8.0, min/max reward -1.0/16.0, entropy 1.73339, value loss 0.01931, policy loss -0.00365\n",
      "Updates 36200, num timesteps 2896080, FPS 917, mean/median reward 8.5/8.0, min/max reward -1.0/16.0, entropy 1.71989, value loss 0.02275, policy loss -0.01392\n",
      "Updates 36300, num timesteps 2904080, FPS 917, mean/median reward 8.4/8.0, min/max reward -1.0/14.0, entropy 1.71473, value loss 0.00873, policy loss -0.01197\n",
      "Updates 36400, num timesteps 2912080, FPS 917, mean/median reward 8.5/9.0, min/max reward -1.0/14.0, entropy 1.71437, value loss 0.00667, policy loss 0.00655\n",
      "Updates 36500, num timesteps 2920080, FPS 917, mean/median reward 10.4/11.0, min/max reward -1.0/17.0, entropy 1.77625, value loss 0.00464, policy loss -0.00446\n",
      "Updates 36600, num timesteps 2928080, FPS 917, mean/median reward 12.1/12.0, min/max reward 4.0/20.0, entropy 1.73455, value loss 0.01747, policy loss 0.02766\n",
      "Updates 36700, num timesteps 2936080, FPS 917, mean/median reward 12.8/12.0, min/max reward 4.0/20.0, entropy 1.77670, value loss 0.02619, policy loss 0.00980\n",
      "Updates 36800, num timesteps 2944080, FPS 917, mean/median reward 12.3/12.0, min/max reward 6.0/20.0, entropy 1.62909, value loss 0.01193, policy loss -0.03654\n",
      "Updates 36900, num timesteps 2952080, FPS 917, mean/median reward 11.6/11.0, min/max reward 2.0/20.0, entropy 1.74298, value loss 0.00714, policy loss 0.00543\n",
      "model saved to  saved_models/a2c/PongNoFrameskip-v4.pt\n",
      "Updates 37000, num timesteps 2960080, FPS 917, mean/median reward 10.2/11.0, min/max reward 2.0/16.0, entropy 1.76696, value loss 0.01858, policy loss 0.02091\n",
      "Updates 37100, num timesteps 2968080, FPS 918, mean/median reward 8.9/8.0, min/max reward 2.0/15.0, entropy 1.67520, value loss 0.02200, policy loss 0.06560\n",
      "Updates 37200, num timesteps 2976080, FPS 918, mean/median reward 10.2/9.0, min/max reward 2.0/17.0, entropy 1.73629, value loss 0.01035, policy loss 0.00344\n",
      "Updates 37300, num timesteps 2984080, FPS 918, mean/median reward 10.1/9.0, min/max reward 2.0/17.0, entropy 1.75053, value loss 0.05559, policy loss -0.14718\n",
      "Updates 37400, num timesteps 2992080, FPS 918, mean/median reward 11.8/12.0, min/max reward 5.0/18.0, entropy 1.69849, value loss 0.06894, policy loss -0.16014\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"#######\")\n",
    "    print(\"WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\")\n",
    "    print(\"#######\")\n",
    "\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    if args.vis:\n",
    "        from visdom import Visdom\n",
    "        viz = Visdom()\n",
    "        win = None\n",
    "\n",
    "    envs = [make_env(args.env_name, args.seed, i, args.log_dir)\n",
    "                for i in range(args.num_processes)]\n",
    "\n",
    "    if args.num_processes > 1:\n",
    "        envs = SubprocVecEnv(envs)\n",
    "    else:\n",
    "        envs = DummyVecEnv(envs)\n",
    "\n",
    "    if len(envs.observation_space.shape) == 1:\n",
    "        envs = VecNormalize(envs)\n",
    "\n",
    "    obs_shape = envs.observation_space.shape\n",
    "    obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "\n",
    "    if args.from_saved_model:\n",
    "        print(\"loading saved model from \", SAVE_PATH)\n",
    "        actor_critic = torch.load(SAVE_PATH)\n",
    "    else:\n",
    "        if len(envs.observation_space.shape) == 3:\n",
    "            actor_critic = CNNPolicy(obs_shape[0], envs.action_space, args.recurrent_policy)\n",
    "        else:\n",
    "            assert not args.recurrent_policy, \\\n",
    "                \"Recurrent policy is not implemented for the MLP controller\"\n",
    "            actor_critic = MLPPolicy(obs_shape[0], envs.action_space)\n",
    "\n",
    "    if envs.action_space.__class__.__name__ == \"Discrete\":\n",
    "        action_shape = 1\n",
    "    else:\n",
    "        action_shape = envs.action_space.shape[0]\n",
    "\n",
    "    if args.cuda:\n",
    "        actor_critic.cuda()\n",
    "\n",
    "    if args.algo == 'a2c':\n",
    "        optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "    elif args.algo == 'ppo':\n",
    "        optimizer = optim.Adam(actor_critic.parameters(), args.lr, eps=args.eps)\n",
    "    elif args.algo == 'acktr':\n",
    "        optimizer = KFACOptimizer(actor_critic)\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space,\\\n",
    "                              actor_critic.state_size)\n",
    "    current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "    def update_current_obs(obs):\n",
    "        shape_dim0 = envs.observation_space.shape[0]\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        if args.num_stack > 1:\n",
    "            current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "    obs = envs.reset()\n",
    "    update_current_obs(obs)\n",
    "\n",
    "    rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "    # These variables are used to compute average rewards for all processes.\n",
    "    episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "    final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "    if args.cuda:\n",
    "        current_obs = current_obs.cuda()\n",
    "        rollouts.cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(args.num_steps):\n",
    "            # Sample actions\n",
    "            value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                      Variable(rollouts.states[step], volatile=True),\n",
    "                                                                      Variable(rollouts.masks[step], volatile=True))\n",
    "            cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "            episode_rewards += reward\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            final_rewards *= masks\n",
    "            final_rewards += (1 - masks) * episode_rewards\n",
    "            episode_rewards *= masks\n",
    "\n",
    "            if args.cuda:\n",
    "                masks = masks.cuda()\n",
    "\n",
    "            if current_obs.dim() == 4:\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "            else:\n",
    "                current_obs *= masks\n",
    "\n",
    "            update_current_obs(obs)\n",
    "            rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks)\n",
    "\n",
    "        next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                                  Variable(rollouts.states[-1], volatile=True),\n",
    "                                  Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "        if args.algo in ['a2c', 'acktr']:\n",
    "            values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                           Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                           Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                           Variable(rollouts.actions.view(-1, action_shape)))\n",
    "\n",
    "            values = values.view(args.num_steps, args.num_processes, 1)\n",
    "            action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "\n",
    "            advantages = Variable(rollouts.returns[:-1]) - values\n",
    "            value_loss = advantages.pow(2).mean()\n",
    "\n",
    "            action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "            if args.algo == 'acktr' and optimizer.steps % optimizer.Ts == 0:\n",
    "                # Sampled fisher, see Martens 2014\n",
    "                actor_critic.zero_grad()\n",
    "                pg_fisher_loss = -action_log_probs.mean()\n",
    "\n",
    "                value_noise = Variable(torch.randn(values.size()))\n",
    "                if args.cuda:\n",
    "                    value_noise = value_noise.cuda()\n",
    "\n",
    "                sample_values = values + value_noise\n",
    "                vf_fisher_loss = -(values - Variable(sample_values.data)).pow(2).mean()\n",
    "\n",
    "                fisher_loss = pg_fisher_loss + vf_fisher_loss\n",
    "                optimizer.acc_stats = True\n",
    "                fisher_loss.backward(retain_graph=True)\n",
    "                optimizer.acc_stats = False\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            (value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef).backward()\n",
    "\n",
    "            if args.algo == 'a2c':\n",
    "                nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % args.save_interval == 0 and args.save_dir != \"\":\n",
    "            \n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            # A really ugly way to save a model to CPU\n",
    "            save_model = actor_critic\n",
    "            if args.cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "                \n",
    "            torch.save(save_model, SAVE_PATH)\n",
    "            print(\"model saved to \", SAVE_PATH)\n",
    "\n",
    "        if j % args.log_interval == 0:\n",
    "            end = time.time()\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "                format(j, total_num_steps,\n",
    "                       int(total_num_steps / (end - start)),\n",
    "                       final_rewards.mean(),\n",
    "                       final_rewards.median(),\n",
    "                       final_rewards.min(),\n",
    "                       final_rewards.max(), dist_entropy.data[0],\n",
    "                       value_loss.data[0], action_loss.data[0]))\n",
    "        if args.vis and j % args.vis_interval == 0:\n",
    "            try:\n",
    "                # Sometimes monitor doesn't properly flush the outputs\n",
    "                win = visdom_plot(viz, win, args.log_dir, args.env_name, args.algo)\n",
    "            except IOError:\n",
    "                pass\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actor_critic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c487f548e1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'actor_critic' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
