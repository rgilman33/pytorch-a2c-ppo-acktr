{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from utils import orthogonal\n",
    "\n",
    "from all_stuff import *\n",
    "\n",
    "from envs import make_env # had to manually add some files into directory for env to reference bc baselines \n",
    "# modules not working right\n",
    "\n",
    "#from storage import RolloutStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.env_name='PongNoFrameskip-v4'\n",
    "        self.seed=1\n",
    "        self.log_dir=''\n",
    "        self.save_dir='saved_models'\n",
    "        self.cuda=False\n",
    "        self.num_stack=4\n",
    "        self.num_steps=5\n",
    "        self.num_processes=16\n",
    "        self.lr=7e-4\n",
    "        self.eps=1e-5\n",
    "        self.alpha=.99\n",
    "        self.max_grad_norm=.5\n",
    "        self.value_loss_coef=.5\n",
    "        self.entropy_coef=.1\n",
    "        self.num_frames=8e6\n",
    "        self.use_gae=False\n",
    "        self.gamma=.99\n",
    "        self.tau=.95\n",
    "        self.save_interval=1000\n",
    "        self.log_interval=100\n",
    "        self.from_saved=False\n",
    "        \n",
    "args = args()\n",
    "\n",
    "SAVE_PATH = \"saved_models/pong_112917.pt\"\n",
    "\n",
    "num_updates = int(args.num_frames) // args.num_steps // args.num_processes\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 80, FPS 149, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.77470, value loss 0.02767, policy loss -0.11150\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "envs = [make_env(args.env_name, args.seed, i, args.log_dir) for i in range(args.num_processes)]\n",
    "\n",
    "if args.num_processes > 1:\n",
    "    envs = SubprocVecEnv(envs)\n",
    "else:\n",
    "    envs = DummyVecEnv(envs)\n",
    "\n",
    "if len(envs.observation_space.shape) == 1:\n",
    "    envs = VecNormalize(envs)\n",
    "\n",
    "obs_shape = envs.observation_space.shape\n",
    "obs_shape = (obs_shape[0] * args.num_stack, *obs_shape[1:])\n",
    "\n",
    "global actor_critic\n",
    "\n",
    "actor_critic = CNNPolicy(obs_shape[0], envs.action_space)\n",
    "\n",
    "if args.from_saved:\n",
    "    print(\"loading saved model from \" + SAVE_PATH)\n",
    "    actor_critic.load_state_dict(torch.load(SAVE_PATH))\n",
    "\n",
    "action_shape = 1\n",
    "\n",
    "if args.cuda:\n",
    "    actor_critic.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space)\n",
    "current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "def update_current_obs(obs):\n",
    "    shape_dim0 = envs.observation_space.shape[0]\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    if args.num_stack > 1:\n",
    "        current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "    current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "obs = envs.reset()\n",
    "update_current_obs(obs)\n",
    "\n",
    "rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "# These variables are used to compute average rewards for all processes.\n",
    "episode_rewards = torch.zeros([args.num_processes, 1])\n",
    "final_rewards = torch.zeros([args.num_processes, 1])\n",
    "\n",
    "if args.cuda:\n",
    "    current_obs = current_obs.cuda()\n",
    "    rollouts.cuda()\n",
    "\n",
    "start = time.time()\n",
    "for j in range(num_updates):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        action = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                                  Variable(rollouts.masks[step], volatile=True))\n",
    "        cpu_actions = action.data.squeeze(1).cpu().numpy()\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, info = envs.step(cpu_actions)\n",
    "        reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "        episode_rewards += reward\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1 - masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "\n",
    "        if args.cuda:\n",
    "            masks = masks.cuda()\n",
    "\n",
    "        if current_obs.dim() == 4:\n",
    "            current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "        else:\n",
    "            current_obs *= masks\n",
    "\n",
    "        update_current_obs(obs)\n",
    "        rollouts.insert(step, current_obs, action.data, reward, masks)\n",
    "\n",
    "    next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                              Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "    values, action_log_probs, dist_entropy = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                   Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                   Variable(rollouts.actions.view(-1, action_shape)))\n",
    "\n",
    "    values = values.view(args.num_steps, args.num_processes, 1)\n",
    "    action_log_probs = action_log_probs.view(args.num_steps, args.num_processes, 1)\n",
    "\n",
    "    advantages = Variable(rollouts.returns[:-1]) - values\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "\n",
    "    action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef\n",
    "    total_loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm(actor_critic.parameters(), args.max_grad_norm)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    if j % args.save_interval == 0 and args.save_dir != \"\":\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if args.cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu() # save THIS one\n",
    "\n",
    "        torch.save(save_model.state_dict(), SAVE_PATH)\n",
    "\n",
    "    if j % args.log_interval == 0:\n",
    "        end = time.time()\n",
    "        total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "        print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   final_rewards.mean(),\n",
    "                   final_rewards.median(),\n",
    "                   final_rewards.min(),\n",
    "                   final_rewards.max(), dist_entropy.data[0],\n",
    "                   value_loss.data[0], action_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def sample(self, x, deterministic):\n",
    "        x = self(x)\n",
    "\n",
    "        probs = F.softmax(x)\n",
    "        action = probs.multinomial()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def logprobs_and_entropy(self, x, actions):\n",
    "        x = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(x)\n",
    "        probs = F.softmax(x)\n",
    "\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "\n",
    "        dist_entropy = -(log_probs * probs).sum(-1).mean()\n",
    "        return action_log_probs, dist_entropy\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        orthogonal(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(32 * 7 * 7, 512)\n",
    "\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "\n",
    "        num_outputs = action_space.n\n",
    "        self.dist = Categorical(512, num_outputs)\n",
    "\n",
    "        self.train()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def act(self, inputs, masks):\n",
    "        value, x = self(inputs, masks)\n",
    "        action = self.dist.sample(x, deterministic=False)\n",
    "        return action\n",
    "\n",
    "    def evaluate_actions(self, inputs, masks, actions):\n",
    "        value, x = self(inputs, masks)\n",
    "        action_log_probs, dist_entropy = self.dist.logprobs_and_entropy(x, actions)\n",
    "        return value, action_log_probs, dist_entropy\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "        relu_gain = nn.init.calculate_gain('relu')\n",
    "        self.conv1.weight.data.mul_(relu_gain)\n",
    "        self.conv2.weight.data.mul_(relu_gain)\n",
    "        self.conv3.weight.data.mul_(relu_gain)\n",
    "        self.linear1.weight.data.mul_(relu_gain)\n",
    "\n",
    "        if self.dist.__class__.__name__ == \"DiagGaussian\":\n",
    "            self.dist.fc_mean.weight.data.mul_(0.01)\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        x = self.conv1(inputs / 255.0)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        value = self.critic_linear(x)\n",
    "        \n",
    "        return value, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space):\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            action_shape = 1\n",
    "        else:\n",
    "            action_shape = action_space.shape[0]\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.observations = self.observations.cuda()\n",
    "        self.rewards = self.rewards.cuda()\n",
    "        self.value_preds = self.value_preds.cuda()\n",
    "        self.returns = self.returns.cuda()\n",
    "        self.action_log_probs = self.action_log_probs.cuda()\n",
    "        self.actions = self.actions.cuda()\n",
    "        self.masks = self.masks.cuda()\n",
    "\n",
    "    def insert(self, step, current_obs, action, reward, mask):\n",
    "        self.observations[step + 1].copy_(current_obs)\n",
    "        self.actions[step].copy_(action)\n",
    "        #self.action_log_probs[step].copy_(action_log_prob)\n",
    "        #self.value_preds[step].copy_(value_pred)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step + 1].copy_(mask)\n",
    "\n",
    "    def after_update(self):\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, use_gae, gamma, tau):\n",
    "        if use_gae:\n",
    "            self.value_preds[-1] = next_value\n",
    "            gae = 0\n",
    "            for step in reversed(range(self.rewards.size(0))):\n",
    "                delta = self.rewards[step] + gamma * self.value_preds[step + 1] * self.masks[step + 1] - self.value_preds[step]\n",
    "                gae = delta + gamma * tau * self.masks[step + 1] * gae\n",
    "                self.returns[step] = gae + self.value_preds[step]\n",
    "        else:\n",
    "            self.returns[-1] = next_value\n",
    "            for step in reversed(range(self.rewards.size(0))):\n",
    "                self.returns[step] = self.returns[step + 1] * \\\n",
    "                    gamma * self.masks[step + 1] + self.rewards[step]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
